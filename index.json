[
  {
    "authors": null,
    "categories": [
      "Data Analysis",
      "Data Science"
    ],
    "content": "  In nearly all cases, the proper way to make predictions on a subset of your data is by holding-out the data you want to predict, training a model on the remaining data, then predicting the outcome on the held-out data using the trained model. The reason is that this procedure ostensibly captures how you would use this model in practice: train the model on all the data you have, then predict for new data where the outcome is unknown. Cross-validation follows this procedure as well. However, that logic (slightly) broke down for an assignment in a class I TA'ed this semester. The confusion was common enough that I thought it warranted some deeper explanation. This post summarizes an answer I gave during office hours and assumes an advanced undergraduate level of statistics background, along with familiarity with Bayesian statistics.\nSuppose you are modeling the lifespan of world leaders. You are given a dataset of Popes, US Presidents, Dali Lamas, Japanese Emperors, and Chinese Emperors. The data include various demographic data: how long they lived, the age and year they assumed office, position held, they year they died, and if they are currently living. The task given to students was to predict how much longer the currently living leaders would survive (5 Presidents, 2 Japanese Emperors, 2 Popes, and 1 Dalai Lama).1 Should you train a model on the deceased leaders, then predict for lifespan for the living leaders? Many students took this approach. The answer, as you can surmise from the fact that this post exists, is no. You can, and should, train a lifespan model using the data from living leaders as well.\nBut first, some notation. In the traditional hold-out method, you pretend you do not know the outcome \\(Y_i\\) for some data \\(i\\) in your hold-out set and you predict that \\(Y_i\\) using \\(X_i\\), covariate information on unit \\(i\\), and \\(\\{Y_j,X_j\\}\\) for \\(j\\) in the observed or training data. That is, you build a model that estimates \\(Y_j\\) given data \\(X_j\\), then apply that model to the hold-out data \\(X_i\\) to get an estimate of \\(Y_i\\). I will refer to the set of all fully observed data \\(\\{Y_j,X_j\\}\\) as \\(Y^{obs}\\).\nTo make this a little more concrete, suppose you believe that lifespan for leaders follows a log-normal distribution, such that \\(log(Y_i) \\sim N(\\beta_0 + \\beta_1 X_{i1} + \\ldots,\\sigma^2)\\). That is, the mean is a linear function of the predictors with a common variance term.2 The form of the model is not particularly important here, just that it has some sort of structure. If we know the parameters \\(\\beta\\) and \\(\\sigma^2\\), then we wouldn't need any training data at all. We know the underlying process and can simply predict lifespans for living leaders using the parameters.\nOf course, we don't know the parameters. But we can learn the parameters from the training data and use them to predict the outcome. In Bayesian statistics this quantity is called the posterior-predictive distribution. We are interested in describing \\(p(Y_i|X_i,Y^{obs})\\), our beliefs or uncertainty about \\(Y_i\\) from the hold-out set given our observed data \\(Y^{obs}\\). Omitting \\(X_i\\) for clarity, this quantity can be analytically expressed as the following:\n\\[p(Y_i|Y^{obs}) = \\int p(Y_i|\\beta,\\sigma^2) p(\\beta,\\sigma^2|Y^{obs}) \\ d\\beta d\\sigma^2\\]\nEssentially, \\(p(Y_i|Y^{obs})\\) is a weighted average of the assumed distribution, in this case log-normal, over the parameter space with parameter weights determined by their posterior density. \\(p(\\beta,\\sigma^2|Y^{obs})\\) is the posterior distribution for the parameters given only the training data. Given samples from the posterior, one can sample from \\(p(Y_i|\\beta,\\sigma^2)\\) to approximate the posterior predictive distribution.\nIf you know nothing about \\(Y_i\\) then the hold-out method is correct and really the only option. You can't learn from observations \\(i\\) where you don't know anything about the outcome.\nHowever, for survival data we do know something about the outcome. We know living leaders will live to at least their current age. The you really want to estimate \\(p(Y_i|X_i,Y^{obs},\\mathbf{Y_i \u0026gt;c_i})\\) where \\(c_i\\) is the living leader's current age. You wouldn't want to predict Jimmy Carter would only live to be 94 because he is currently 96. The expression from above becomes the following:\n\\[p(Y_i|Y^{obs},Y_i\u0026gt;c_i) = \\int p(Y_i|\\beta,\\sigma^2,Y_i\u0026gt;c_i) p(\\beta,\\sigma^2|Y^{obs},Y_i\u0026gt;c_i) \\ d\\beta d\\sigma^2\\]\nThe key difference is the term \\(p(\\beta,\\sigma^2|Y^{obs},Y_i\u0026gt;c_i)\\). This is still a posterior distribution but it is not the same posterior distribution as before because it includes the information on additional leaders who have lived at least \\(c_i\\) years. There are five currently-living Presidents and the fact that they have reached their current ages should inform your beliefs about word leader life expectancy. If you use the hold-out method, you might predict a currently-living leader will die in the past, which is obviously wrong. If you simply force your predictions to predict time-of-deaths in the future, then you have trained your model on incomplete data and used the wrong posterior distribution. You modeled \\(Y_i|Y^{obs},Y_i\u0026gt;c_i\\) but learned your parameters \\(\\beta\\) and \\(\\sigma^2\\) only from \\(Y^{obs}\\) rather than \\(Y^{obs}\\) and \\(Y_i\u0026gt;c_i\\). I've used Bayesian formulations here because they provide nice ways to estimate survival models and make the distinction between the input data clear, but the logic applies to any estimation of future event times.\nHold-out predictions and cross-validation procedures can be deceptively complex. Your predictive model should replicate how you will actually use it in practice. If you want to predict event times in the future, you should include in your model that those events have not happened yet. Including that information can be hard and may require a more complex estimation of the parameters given the data because the likelihood is now a product of densities \\(p(Y_j)\\) and survival functions \\(P(Y_i\u0026gt;c_i)\\).3 But it is certainly the “correct” way to do it because it includes all of the data currently available.\n The actual assignment had more components and is available here.↩\n The students were tasked with using a more complicated model that is generally better for survival analysis but too complicated for exposition here. The assignment was based on expanding this paper: Stander, J., Dalla Valle, L., and Cortina-Borja, M. (2018). A Bayesian Survival Analysis of a Historical Dataset: How Long Do Popes Live? The American Statistician 72(4):368-375.↩\n Of course, if you are a Bayesian, that combination is trivial.↩\n   ",
    "date": 1607385600,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1607385600,
    "objectID": "54c73f064de3ea153cbce31314d80c2d",
    "permalink": "https://g-tierney.github.io/post/survival_hold_out_writeup/",
    "publishdate": "2020-12-08T00:00:00Z",
    "relpermalink": "/post/survival_hold_out_writeup/",
    "section": "post",
    "summary": "When predicting event times in the future, you should include that information in the training data.",
    "tags": [
      "Survival Analysis"
    ],
    "title": "Why you shouldn’t “hold-out” data in survival-model predictions",
    "type": "post"
  },
  {
    "authors": null,
    "categories": [
      "Data Analysis"
    ],
    "content": "  The Project One fateful day while I was bored during a lecture, I decided to sign up for email messages from each Senate campaign during the 2018 election cycle.1 A lot of people study the impact of political advertisements on various outcomes, and I thought some interesting trends might emerge in the email blasts that campaigns send out.\n The Data I signed up using a new email address, and only filled out the required fields to get on mailing lists. I used my real name, the zip code 00000, and a phone number of all zeros. The data for what information I gave to each campaign is on Github. I felt kind of bad signing up for volunteer lists with false information, which were the only email option for some campaigns. The data turned out to be pretty interesting though, so I think next cycle for the presidential election I will try to get on a more comprehensive set of emails by signing up for the House races too and providing zip codes and phone numbers in the relevant district.2\nI found the candidates and campaign websites from RealClearPolitics’s Senate map. I started signing up for emails on 6/6/2018, but didn’t sign up for every senate race until 10/9/2018. In the last month before the election (October 6 through November 6 inclusive) I received 2,650 emails from 50 unique campaigns. Some campaigns did not have an option to signup for an email list on their website and some may have filtered out my email address because the zip code and/or phone number were clearly not accurate. Much of the analysis below compares emails from Democrats and Republicans, so I further filter the emails down to races where I received at least one email from both party’s candidates. The final number is 2,397 emails and 44 campaigns. Most analysis uses this sample, and I will specify when that is not the case.\n Who is sending emails and when? I received emails from both parties in the following states: AZ, FL, IN, MA, MD, MI, MN, MO, MS, ND, NE, NJ, NV, NY, OH, PA, TN, TX, USA, UT, VA, VT, WA, WY. A notable omission is West Virginia, where I only received emails from Joe Manchin. Below I show the number of emails I received each day from each party.\nWhat immediately jumped out to me was that the democratic candidates send significantly more emails (and for some reason campaigns send the fewest emails on Wednesdays). Next, I tally the number of emails I received from each candidate, and show the races where I received at least 100 emails in total.\n  State  Party  Campaign  Total Emails      NV  D  Jacky Rosen  352    NV  R  Dean Heller  147    FL  D  Bill Nelson  237    FL  R  Rick Scott  7    MO  D  Claire McCaskill  185    MO  R  Josh Hawley  30    ND  D  Heidi Heitkamp  176    ND  R  Kevin Cramer  29    AZ  D  Kyrsten Sinema  96    AZ  R  Martha McSally  69    IN  D  Joe Donnelly  106    IN  R  Mike Braun  50    USA  D  DNC  59    USA  R  RNC  73    MN  D  Amy Klobuchar  19    MN  D  Tina Smith  62    MN  R  Karin Housely  38     Jacky Rosen, Bill Nelson, Claire McCaskill, Heidi Heitkamp, and Joe Donnely (all democrats) sent over 100 emails during the relevant time-frame. Dean Heller was the only republican who sent me over 100 emails. In general, Democrats sent more emails than their Republican opponents. However, I certainly would not be surprised if my sample was biased. People who signed up with in-state addresses probably received more emails than I did. I don’t know how sophisticated campaigns are with targeting their emails, but I would be shocked if they did not focus efforts like Get Out the Vote campaigns on people with addresses in their district. I do wonder, though, if there is a connection between the fundraising strategies and email strategies of each party. If Democrats rely more on smaller donations from many individuals, they might need to send more emails to everyone who expresses interest in their campaign. Republicans who either self-fund or court fewer donations from wealthier individuals might simply not have much to gain from emailing out-of-state individuals.\n Email Content Next, I will analyze the content of the emails. The word clouds above show each word sized proportionally to the amount of times it was used in the email. Both party’s emails frequently used words like senate, vote, and fight, but some differences are already apparent. Here I do unfortunately need to filter the dataset down more. Many of the emails that were sent came in a format that did not download or parse into human-readable text well. I tried to extract the text from all emails, but some (particularly ones with odd formatting or with pictures of text) I could not parse properly. The number of emails analyzed in this section is only 2,043.\nWord clouds are a useful visualization, but I will use a statistical technique, the relative risk ratio, to characterize the difference in word-usage between Republican and Democratic emails.\nThe chart above merits some further explanation. For each word, I calculated the proportion of Democratic emails that used the word and the proportion of Republican emails that used the word. Then, I took the ratio of those two quantities, often referred to as the relative risk ratio. To put everything on a comparable scale, if Republican emails used the word more frequently, I multiplied the proportion by -1 and took the inverse. So if the ratio is equal to R and positive, then Democratic emails used the word R times more frequently. If the ratio is equal to R and negative, then Republican emails used the word R times more frequently. I show the 15 words with the largest ratio for each party.3\nSomething that I noticed quickly is that election-specific terms from races where the Democrat sent many more emails than the Republican have high risk ratios: “Las Vegas”, “Scott” (Bill Nelson referring to his opponent Rick Scott), and “FL” are all in the top 15 for democrats. “ActBlue” is an organization that helps Democrats fundraise. “politicalemaild” is a truncated version of the email address I provided to campaigns.\nI was not surprised that the words “borders” and “conservative” are in the top for Republicans, but I was quite surprised by “web” and “website” showing up in the top 15. “Liberal” is used frequently as a pejorative by Republicans, but apparently Democrats do not use the word anywhere near as frequently when messaging their own supporters.\nOf course, words that are used by one party and never used by the other will have a risk ratio of plus or minus infinity. The problem with looking at all of those words is that they often are spelling or parsing errors that happen once or twice for one party and never for the other. To account for that, I show only the 10 most frequently used words that are never used by the opposing party.\n   Democratic   Republican     Word  Emails Using Word  Proportion Using Word  Emails Using Word  Proportion Using Word      youd  625  0.287  0  0.000    mitch  307  0.141  0  0.000    mcconnell  299  0.138  0  0.000    environment  262  0.121  0  0.000    fivethirtyeight  224  0.103  0  0.000    silvers  217  0.100  0  0.000    nelson  212  0.098  0  0.000    melbourne  210  0.097  0  0.000    whitmire  210  0.097  0  0.000    floridas  198  0.091  0  0.000    inherit  0  0.000  74  0.079    nrcc  0  0.000  66  0.071    chuck  0  0.000  62  0.067    complaints  0  0.000  60  0.064    devoted  0  0.000  60  0.064    conservatives  0  0.000  57  0.061    suggestions  0  0.000  57  0.061    replying  0  0.000  56  0.060    schumer  0  0.000  54  0.058    charitable  0  0.000  53  0.057     Again, “Florida’s” and “Nelson” show up as top democratic words never used by republicans because I received many emails from Bill Nelson’s campaign and almost none from Rick Scott’s. I was surprised that no Republican emails used the word “you’d.” One potential explanation is that Democrats use more personal apeals in their emails, which is further evidenced by the fact that Democrats used my email address more than republicans. Democrats referenced “Silver’s” “FiveThirtyEight” website in over 200 emails, while Republicans never mentioned either him or the website.\nBesides the words that are used most differently, I was also curious how some specific words showed up in emails. Below I tabulate the usage of a few select words. Specifically, I was interested in which politicians get mentioned by each party, some campaign issues, and party signifiers.\n   Democratic   Republican      Word  Emails Using Word  Proportion Using Word  Emails Using Word  Proportion Using Word  Signed Risk Ratio      trump  365  0.168  183  0.196  -1.170    obama  27  0.012  27  0.029  -2.333    pelosi  0  0.000  38  0.041  NA    mcconnell  299  0.138  0  0.000  NA    caravan  0  0.000  5  0.005  NA    kavanaugh  54  0.025  60  0.064  -2.592    radical  4  0.002  68  0.073  -39.655    conservative  3  0.001  136  0.146  -105.745    liberal  1  0.000  112  0.120  -261.253    progressive  25  0.011  14  0.015  -1.306     While Trump and Obama were mentioned in about 30% of emails by both Democrats and Republicans, congressional leaders were almost exclusively mentioned by the opposing party. Democrats mentioned Mitch McConnell in about 16.5% of their emails, while he was never mentioned by Republicans. Republicans mentioned Nancy Pelosi in about 9% of their emails, while she was mentioned in only 0.5% of democratic emails. This seems to lend truth to the impression I got from news coverage that Republicans ran against Pelosi and Democrats ran against McConnell.\n Email Sentiment How did the sentiment of emails differ by party? Here I will rely on some of the built-in dictionaries in the tidytext package in R. Sentiment analysis generally refers to matching each word with some sentiment, either on a positive or negative scale or to some (short) list of topics. A review of the dictionaries that I will be using is here.\nThe simplest way to find the sentiment of a document is to use a dictionary that maps words to whether they express a positive or negative sentiment, give each positive word a weight of +1 and each negative word a weight of -1, then find the average for each document. The chart below does just that. Note that the bounds are at plus or minus 1 because I only take the average over words with a sentiment attached to them.\nI plotted a density histogram because I received many more democratic than republican emails. The total area of the bars is scaled to sum to one for each party. The results look pretty interesting. For both parties, the distribution of sentiment is concentrated at 0.5, which corresponds to emails that use 75% positive words and 25% negative words. Democratic emails are more concentrated at this value, and the sentiment of Republican emails is more spread out with additional concentrations at plus or minus 1 (all positive or all negative words).\nNext I try a slightly more complicated sentiment analysis. I use a dictionary that maps words to (potentially multiple) of the following sentiments: trust, fear, negative, sadness, anger, surprise, positive, disgust, joy, and anticipation. Then, I calculate the proportion of words with the given sentiment among words that had any sentiment for each email and plot a histogram of the results. There are a lot of charts below, but I think they all tell the same story. The overall sentiment of the words used by each party are extremely similar. If all you knew about a campaign email was the sentiment score reported below, it would be extremely hard to correctly guess the party of the candidate.\n Classification Overall, it looks like certain words can give away the political alignment of the email, but the sentiment cannot. However, that conclusion just comes from examining the histograms. A more sophisticated method to measure how well sentiment can identify political text is to fit a prediction model, then test how well it works on a held-out set. If the information about sentiment improves predictive power, then there is evidence that the parties speak to their supporters differently. A recent working paper by Gentzcow, Shapiro, and Taddy measure political polarization by how accurately speeches on the floor of Congress can identify the partisan alignment of the speaker.4 The classification accuracy results here can be interpreted similarly. I will use the two most straightforward techniques that I know of for this task: logistic regression and leave-one-out cross validation (LOOCV).\nLogistic regression is a standard method to fit a model to a binary outcome. What I want to do is estimate the probability that an email is from a Democrat as a function of the sentiment scores. A linear relationship will not work because probabilities need to be between zero and one. Logistic regression assumes that the log of the odds of an event (the probability of an email being Democratic divided by the probability of the email being Republican) is a linear function of the explanatory variables. Because of this change in perspective, interpreting the coefficients of a logistic regression is a little bit harder.5 It is still the case, however, that positive coefficients mean an increase in the explanatory variable is associated with an increase in the probability that the dependent variable equals one, so the interpretation of the sign of the coefficient remains the same.\nLeave-one-out cross-validation is an evaluation method where a model is fit on all data points except one, then the model is asked to predict the value at the held-out data point, and finally the prediction and observed value are compared. In my application, I code the event that an email comes from a democratic campaign as a one and the event that an email comes from a republican campaign as a zero. If the model predicts that an email is from a Democratic campaign with probability greater than 0.5, I classify it as a Democratic email and as a Republican email otherwise. The LOOCV error rate is the number of miss-classified emails divided by the number of emails.\nThe explanatory variables I use are all the variables charted above (the average of positive and negative sentiment, called “Email Sentiment”, and each of the categorical sentiments) and the number of sentiment words in each email. To see if results were being driven by a single sentiment, I fit a univariate model with each term individually and a full model with all of the variables included. The results of the univaraite regressions are shown below.\n  Model  Coefficient  Intercept  LOOCV Error Rate      Email Sentiment  0.084**\n(0.03)  0.663***\n(0.016)  30.4%    Trust  -0.735***\n(0.086)  1.018***\n(0.039)  28.4%    Fear  -0.454***\n(0.126)  0.746***\n(0.017)  30.3%    Negative  0.391***\n(0.097)  0.596***\n(0.027)  30.4%    Sadness  0.989***\n(0.142)  0.569***\n(0.021)  30.4%    Anger  0.023\n(0.128)  0.693***\n(0.019)  30.4%    Surprise  0.228\n(0.145)  0.67***\n(0.019)  30.4%    Positive  -1.26***\n(0.073)  1.263***\n(0.034)  27.7%    Disgust  -0.611**\n(0.187)  0.73***\n(0.015)  30.2%    Joy  -1.692***\n(0.109)  0.926***\n(0.018)  27.1%    Anticipation  -0.883***\n(0.107)  0.871***\n(0.024)  30.1%    Number of Sentiment Words  0\n(0.001)  0.692***\n(0.024)  30.4%      Note:      * p\u0026lt;0.05; ** p\u0026lt;0.01; *** p\u0026lt;0.001     While the error rates appear low, they actually are not much better than random chance. Of the 1,880 emails with sentiment data, 69.6% of them are from democratic campaigns. So if you just guessed that every email was Democratic, you would be wrong 30.4% of the time. An equivalent interpretation is that if you randomly classified 69.6% of the emails as Democratic and the rest as Republican, you would expect to have an error rate of 30.4%. Even for some of the sentiments with statistically significant coefficients, their prediction accuracy is no better than random chance. The full model, however, does have some predictive power. The results are shown below.\n  Variable  Coefficient      Email Sentiment  0.99***\n(0.2)    Trust  -6.511***\n(0.704)    Fear  -8.376***\n(1.04)    Negative  -2.256*\n(0.945)    Sadness  6.867***\n(1.148)    Anger  3.013**\n(1.117)    Surprise  1.201\n(1.106)    Positive  -7.824***\n(0.68)    Disgust  -7.099***\n(1.263)    Joy  -3.985***\n(1.089)    Anticipation  -1.123\n(0.773)    Number of Sentiment Words  -0.002\n(0.004)    Intercept  8.438***\n(0.646)      Note:      * p\u0026lt;0.05; ** p\u0026lt;0.01; *** p\u0026lt;0.001. LOOCV Error Rate: 21.6%     The error rate for the full model is 21.6%, an improvement over random chance by a factor of about 2/3. By using all of the sentiments together, emails can be classified better than by simply guessing that every email is democratic.\nEmails with higher positive average sentiment, or more sadness and anger words are more likely to be democratic emails. Emails with more trust, fear, negative, positive, disgust, or joy words are more likely to be from republican candidates. Democrats might tend to describe the previous Congress with words that convey anger or sadness, while Republicans would use positive, joy, or trust words. The fact that Republican emails also use more disgust and fear words is evidence against that explanation though, or at least is evidence that the truth is more complicated.\n Further Work The classification and cross-validation methods that I used are certainly simpler versions of where the state of the art is. I would not be surprised if more information could be extracted from the text or the sentiments with more sophisticated tools. Something that I haven’t explored, but could be quite interesting, is using the text itself rather than the sentiment to classify emails. The difficulty is that text data is very high dimensional: there were far more unique words used than emails sent. Regression with more variables than observations tends to overfit the estimation data and produce poor out-of-sample predictions. There are also some words that are very predictive, but not predictive in an “interesting” way. Emails always mention the candidate’s name, frequently include a link to the campaign’s donation page or list their mailing address. Clearly, you can immediately know the party of the sender from those features. But those predictions are also not very interesting.\nWith unlimited time and resources, what I would like to do is extract from the emails some measures of the general language each party uses and then use those features to classify the email. If those features classify accurately, then there is evidence for divergence in how the parties speak to their supporters. That result has implications for trying to convince people to change their political beliefs and how to talk to people from across the isle. Presenting convincing arguments to partisans of each party probably requires speaking their language in some sense, and campaign emails are potentially a useful data source for learning that language.\n  This project may have been entierly inspired by the fact that I really liked the title.↩\n The code and data are on Github here: https://github.com/g-tierney/political_emails.↩\n I also removed all non-alphanumeric characters from each word.↩\n The full article can be found here.↩\n Technically, a one unit increase in the explanatory variable increases the odds that an event occurs by \\(e^\\beta\\) where \\(\\beta\\) is the estimated logit coefficient.↩\n   ",
    "date": 1544140800,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1544140800,
    "objectID": "10b87e0ddafbca04910ded90c182319b",
    "permalink": "https://g-tierney.github.io/post/political_emails/",
    "publishdate": "2018-12-07T00:00:00Z",
    "relpermalink": "/post/political_emails/",
    "section": "post",
    "summary": "Text analysis of 2018 campaign emails. ",
    "tags": [
      "Politics"
    ],
    "title": "What About the Emails?",
    "type": "post"
  },
  {
    "authors": null,
    "categories": [
      "Article Review"
    ],
    "content": " Judea Pearl’s The Book of Why: The New Science of Cause and Effect I was honestly surprised by how much I liked this book. It covers Pearl’s new work on causal diagrams, directed graphs where each node represents a variable and edges point in the direction of the causal effect. His claim is that these graphs make previously “squishy” arguments about identification mathematically robust, simplify many hard-to-explain causal concepts, and will be useful in building strong artificial intelligence because the graphs are machine-readable, unlike other causal identifying assumptions.\nFor a little bit of background, usually when a researcher makes a causal claim about observational data (not a randomized control trial), they also specify some assumptions about how the variables need to relate to one another for the result to be causal not just correlational. Often these are justified by subject area knowledge or, when possible, other data analyses. Pearl’s causal diagrams, he claims, are easier to understand and interpret than written out descriptions of the model and assumptions.\nWhat I found particularly entertaining in this book were the shots Pearl took at other practitioners, notably statisticians and economists, who he claims have thrown up their hands and abandoned causation as too difficult to prove. I can certainly sympathize with this view, particularly among economists, who I think are often too reluctant to make causal claims without randomized experiments.\nHowever, throughout the book I constantly expected the next chapter to be about how to draw these diagrams. The examples he used were often related to smoking and lung cancer, where we have a very strong existing understanding of the causal mechanisms. But the diagrams seem much less helpful when you are not certain about which edges exist (and in which direction), or even what variables should be represented. The answer might delve too deep into epistemology (how do we “know” an empirical relationship exists), but it seems like a central question to the utility of these diagrams. The data analyst might rely on experts for the diagram, but I think many of the issues in causal inference come not from the ability to explain the causal assumptions but rather in justifying those assumptions.\nOf course if an expert hands you a list of relationships that are accepted conventional wisdom in the field, a data analyst could do the causal analysis. But that never really happens outside of a few contrived subject areas. Particularly in social sciences, almost no relationships are accepted as true. Social structures even change over time, so the fact that some correlation held in the past is not always a good justification for using it in a causal diagram about current trends. To me, its always seemed easiest to make causal inferences when you have a strong understanding of the subject area. Going from a list of beliefs to a directed graph might help formalizing the mathematical definitions of cause and effect, but it really doesn’t seem like it would help much in practice because the hard part is getting that list and quantifying uncertainty about the list.\nAn area where I do understand how the diagrams are useful is when Pearl discussed how to teach causation to machines. A list of beliefs is quite hard for a computer to understand, but a directed graph referring to how each variable causes changes in others does sound very machine-interpret-able. The problem of generating the list still seems hard, but the notion of how do you tell a computer that X causes Y and not the reverse was pretty interesting.\n John Carreyrou’s Bad Blood: The Theranos Story This was a really fascinating rundown of how a well-regarded start-up collapsed. The short of it is that the promises made by the founder and CEO Elizabeth Holmes about being able to run hundreds of medical tests on a single drop of blood were lies. Employees who raised concerns that the tests were not accurate or that the product just didn’t work were told that they weren’t ``team players,’’ fired, and forced to sign non-disclosure agreements. Through a mixture of deceit (showing “live” demos that were actually recordings of results), strong branding, and an idea that, if it had worked, would genuinely have been revolutionary, she convinced senior executives and officials at a variety of institutions to buy into her company and her personally. When results did not look good or deadlines were not met, the executives made excuses or accepted the delays because they believed in the grand vision of revolutionizing health care.\nThe story is well chronicled in the book and the popular press, so I’ll just highlight one of my takeaways that I haven’t seen discussed elsewhere. A lot of the information John Carreyrou gathered would have been impossible without Theranos hiring some wealthy and well-connected Stanford graduates. The former employees who spoke to Carreyrou had to bear significant legal risk. Theranos had paid David Boise, one of the most feared attorneys in the country, with company shares and his law firm vigorously enforced the NDAs employees had signed. They hired investigators to follow ex-employees who they suspected were talking to journalists. Theranos both threatened to sue and actually sued potential whistleblowers. One of Carreyrou’s main informants was a Stanford graduate who had wealthy parents and was the grandson of a former Secretary of State. He (his parents) had the financial resources to weather the legal attacks until the story broke, while other whistleblowers (quite rationally) backed out after threats of litigation.\nExisting whistleblower statutes were certainly not enough to protect the former Theranos employees who had concerns. It should not be only privileged white men who can take the risk of reporting the companies that they work for.\n ",
    "date": 1541030400,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1541030400,
    "objectID": "ba4f0f2e3a51bb7b05757490593f7707",
    "permalink": "https://g-tierney.github.io/post/2018_10_books/",
    "publishdate": "2018-11-01T00:00:00Z",
    "relpermalink": "/post/2018_10_books/",
    "section": "post",
    "summary": "Some thoughts on how to build causal graphs and the role of privilege in taking down Theranos. ",
    "tags": [
      "Causal Inference"
    ],
    "title": "Reviews: The Book of Why and Bad Blood",
    "type": "post"
  },
  {
    "authors": null,
    "categories": [
      "Article Review",
      "Data Science"
    ],
    "content": " I came across two articles recently that I thought spoke to each other in an interesting way. The first was a New York Times piece about the failings of data science firms who try to identify school shootings before they happen by social media posts. The second was a Vox article about how a crisis counseling hotline successfully used data science to flag callers who are at higher risk of suicide or self-harm.\nI have some specific thoughts on both, but I think the comparison between the two articles shows why a data-driven approach is helpful in one case and not the other.\nA Failure: Predicting School Shootings “Could Monitoring Students on Social Media Stop the Next School Shooting?” by Aaron Leibowitz.\nThis article reviews the services that several companies are providing to school districts by monitoring public posts by students on social media. These companies usually scrape data from all posts in a geographic region around the school. “Rather than asking schools for a list of students and social media handles, the companies typically employ a method called “geofencing” to sweep up posts within a given geographic area and use keywords to narrow the pool.”\nHowever, as you can imagine, this wide of a net ends up flagging posts from many people unaffiliated with the school. One school in Ohio was warned about someone posting “There’s three seasons: summer, construction season and school shooting season.” Further investigation discovered that the poster was from Wisconsin not Ohio. Another school that hired one of these firms was close to a liquor store, and the firm couldn’t separate tweets about the store and the school. In general, the problem seems to be that in the available data, any signals, if they exist, are swamped by the noise.\nThis monitoring also brought up some philosophical questions about what out-of-school actions should have in-school consequences. A case of a school that hired an outside firm to review students’ posts on social media expelled 14 students. Some of the allegations are described below:\n One student had been accused of “holding too much money” in photographs, an investigation by the Southern Poverty Law Center found, and one was suspended for an Instagram post in which she wore a sweatshirt with an airbrushed image of her father, a murder victim. School officials said the sweatshirt’s colors and the student’s hand symbol were evidence of gang ties, according to the investigation.\n I can understand an administrator’s desire to punish students for some out-of-school actions, but these seem like they’ve gone too far. School officials who suddenly have access to all public activity of their students need to think harder about what kind of actions they want to police. Especially if they want to continue to monitor students’ activities for more serious transgressions, officials need to be tolerant of activities they may disapprove of to keep that information channel open. I’m sure that many fewer students made any public posts after these expulsions.\n A Success: Identifying High-Risk Callers to a Crisis Hotline “How data scientists are using AI for suicide prevention” by Brian Resnick\nA more heartening case is this article about the data science team at Crisis Text Line. CTL provides crisis counseling via text message to anyone who requests it. Certain events cause dramatic increases in demand for their services; Robin Williams’s suicide and the 2015 terrorist attack in Paris are two examples. The volunteers working at the time cannot handle everyone at once, so they used machine learning to prioritize incoming requests based on the text message rather than using the order the requests came in. The words most predictive of an active rescue (when 911 is called) were the names of household drugs like Advil or Ibuprofen, even the crying face emoji was more predictive than the word “suicide” and other words like “cut” and “kill” that the company had thought would be good predictors previously.\nSomething I wondered throughout this piece was what really was the “machine learning” being used and did it really rise to the level of artificial intelligence? It sounds like the analysis could have been simply computed by a simple logistic regression of active rescue on indicators for which words were used in the first message. It sounds a bit pedantic, but I think overuse of buzzwords like machine learning and AI discourage people who would have valuable insights or could produce data analysis of similar rigor and results from looking into these problems. For a longer read on how data is being used (and not used) in the counseling profession, check out this Atlantic piece that was linked in the Vox article.\n What was the difference? Both CTL and the school shooting programs were trying to predict individual behavior from limited textual data. I think the difference is that the event they wanted to detect was much more frequent and observable in the studied population.\nMy intuition is that the proportion of school shooters among everyone living near a school is significantly smaller than the proportion of callers to a crisis hotline who need an active rescue. The most credible estimates are that in the 2015-2016 school year there were 11 to 29 school shootings across the country. The data on social media posts by the shooters has to be even more scarce. Statistical methods to identify which features are predictive of a given event need data from when the event does and does not occur. The school security firms, lacking many data on what shooters post before they bring a gun to school, ended up simply referring “violent-sounding” messages to school officials, without being able to specify how likely that person was to actually be violent at school. That determination was left up to school officials who, given discretion, appear to have made some poor choices about what kinds of messages merited a response. I would not be surprised if part of the justification school administrators had in mind was that the data science firm flagged the message because they thought the person would be violent at school. When in reality, the firm isn’t doing any data-based prediction on what messages correlate with actual violence.\nCTL, however, had a clear outcome variable and data on both callers who did and did not need active rescue. They were able to build a statistical model and identify predictive features of the incoming messages to effectively allocate resources. CTL had the appropriate labeled data, but the schools only had a limited selection of messages from students who had not yet (and might never be) violent at school.\n ",
    "date": 1537401600,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1537401600,
    "objectID": "050d7d7c96c02dc4fd01804915268865",
    "permalink": "https://g-tierney.github.io/post/2018_09_mental_health/",
    "publishdate": "2018-09-20T00:00:00Z",
    "relpermalink": "/post/2018_09_mental_health/",
    "section": "post",
    "summary": "I review two applications of predictive modeling in mental health, one successful and the other not: identifying school shootings before they happen and prioritizing callers to a crisis hotline. ",
    "tags": [
      "Mental Health",
      "Data Science"
    ],
    "title": "Data Science in Mental Health",
    "type": "post"
  },
  {
    "authors": null,
    "categories": [
      "Data Science",
      "Methods"
    ],
    "content": "  Introduction I recently encountered a problem that had a surprisingly elegant solution. I struggled a lot with solving this issue, so hopefully in writing this post I can save someone else the trouble! For reasons that are irrelevant, I wanted to track the performance of youth fencers across time. National ranking lists are posted each year, but the fencers’ names frequently change from year to year. My solution was to create a dataset of all sets of two names, mark which pairs were matches, then pick a single name to use for each set of names that matched against each other. My mistaken belief was that the second step would be the hardest, but actually the third was the most difficult.\n The Problem The data I had were end-of-season rankings for youth fencers by age, gender, and weapon. The only identifier to link fencers across years was their name and year of birth. However, the names are notoriously difficult to standardize. Some years a kid goes by John, others its Jonathan, and maybe a third time its John Smith IV. Numeric suffixes seemed quite overrepresented among fencers relative to the general population. Some Asian fencers would go by a phonetic spelling of their given name in one year then use an English name in the next.\n Solution Part 1: The (Surprisingly) Easy Part I joined the list of names (along with the other less granular identifiers) to itself on the less granular identifiers to get a list of all pairs of names that had the same age, gender, and weapon. I removed pairs whose last names required more than three deletions, insertions, or single-letter transformations to match (the Levenshtein distance). I then tasked two undergraduate RAs to independently inspect each pair and mark the ones that could be the same name, then resolve any discrepancies in the matches they found.\nI had thought this step, reviewing each pair of names, would be the most time-consuming, but it actually was rather quick. The 11,211 were reviewed and mismatched in less than 8 hours of work per RA. A snapshot of the final dataset is shown below.\n  match  name_key  name_key2  gender  weapon  bthyear      0  aaron.ahn  albert.park  Male  foil  1997    0  aaron.ahn  albert.park  Male  foil  1997    0  aaron.ahn  bin.ahn  Male  foil  1997    0  aaron.ahn  royce.wang  Male  foil  1997    0  aaron.ahn  mikolaj.bak  Male  foil  1997    0  aaron.ahn  eric.zhang  Male  foil  1997      Solution Part 2: The Hard Part After this dataset of matches was created, I needed to identify for each name, all other names that matched with it, then pick one of those names to use as the “real” name. The operations required to do this were surprisingly challenging. Each name appeared in both name_key columns, so any grouping had to be done on two variables. It also requires a consistent operation that will select the same “real” name for each name within the matches. I was struggling to implement this solution on the rectangular dataframe. I needed to group by names in two variables, spread unique values of two variables into multiple columns, then consistently select one of those entries. Certainly, there is a way to do this, but it was not intuitive to me and I suspect it would be quite a slow operation.\nEventually, I realized that instead of trying to operate on the data as a matrix with variables in columns and observations in rows, I should treat the data as a graph. Each name was a node, and edges represent names that were matched. Each connected graph within the disconnected graph of all names represented a single “real” name. To extract what I needed, I just had list each node and which graph it was in, then arbitrarily pick one node from each graph to be the “real” name.1\nFortunately, people who write much better R code than me have developed tools to operate on graphs quickly and efficiently. What took me hours to (unsuccessfully) do on a rectangular dataframe took approximately 30 minutes using graph operations. A quick visualization of the graph is below.\ngraph_plot \u0026lt;- suppressWarnings(graph.data.frame(name_matches[1:100,] %\u0026gt;% filter(match == 1) %\u0026gt;% select(name_key,name_key2),directed = F)) #remove duplicated edges graph_plot \u0026lt;- graph_plot %\u0026gt;% simplify() set.seed(0515) #fix the position of nodes on the plot ggnet2(graph_plot,label = T,layout.exp=2,color = \u0026quot;lightskyblue2\u0026quot;) + ggtitle(\u0026quot;Sample Name Network\u0026quot;) + theme(plot.title = element_text(hjust = .5,face = \u0026quot;bold\u0026quot;,size = 15)) Now here is the code itself. The full script and example data are available at this Github repository.\nlibrary(igraph) #add other identifiers to node names combine_ids \u0026lt;- function(...){ str_c(...,sep = \u0026quot;_\u0026quot;) } name_matches \u0026lt;- name_matches %\u0026gt;% rowwise() %\u0026gt;% mutate(name_key = combine_ids(name_key,gender,weapon,bthyear), name_key2 = combine_ids(name_key2,gender,weapon,bthyear)) #turn data into a graph graph \u0026lt;- graph.data.frame(name_matches %\u0026gt;% filter(match == 1) %\u0026gt;% select(name_key,name_key2),directed = F) dg \u0026lt;- decompose.graph(graph) #list names of verticies grouped by connected graphs name_links \u0026lt;- map(dg,function(x){V(x)$name}) #combine into a single dataframe to merge make_df \u0026lt;- function(list_element){ vec \u0026lt;- unlist(list_element) std_name \u0026lt;- str_split(vec[1],\u0026quot;_\u0026quot;,simplify = T)[1] data.frame(name_key_combined = vec,std_name = std_name,stringsAsFactors = F) } name_standardizations \u0026lt;- do.call(rbind,map(name_links,make_df)) #spread identifiers back into multiple columns name_standardizations \u0026lt;- name_standardizations %\u0026gt;% separate(name_key_combined,into = c(\u0026quot;name_key\u0026quot;,\u0026quot;gender\u0026quot;,\u0026quot;weapon\u0026quot;,\u0026quot;bthyear\u0026quot;),sep = \u0026quot;_\u0026quot;) name_standardizations[1:7,] %\u0026gt;% kableExtra::kable(format = \u0026quot;html\u0026quot;) %\u0026gt;% kableExtra::kable_styling(bootstrap_options = \u0026quot;striped\u0026quot;,full_width = F)   name_key  gender  weapon  bthyear  std_name      abagael.a.buckborough  Female  sabre  1999  abagael.a.buckborough    abagael.r.buckborough  Female  sabre  1999  abagael.a.buckborough    abby.buckborough  Female  sabre  1999  abagael.a.buckborough    abby.emerson  Female  foil  1987  abby.emerson    abigail.emerson  Female  foil  1987  abby.emerson    abby.schifferle  Female  foil  1988  abby.schifferle    abigail.schifferle  Female  foil  1988  abby.schifferle     The lesson I learned from this experience was the importance of taking a step back from a difficult problem and approaching it from a different angle. Not thinking of data as a matrix of numbers was instrumental to solving this particular problem and is likely key to solving many others.\n  Incidentally, I think the reason I came up with this idea was I was researching professors at Ph.D. programs I was accepted to, and one of them, Rebecca Steorts, mentioned research on record linkage using graphs in her research interests.↩\n   ",
    "date": 1535155200,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1535155200,
    "objectID": "75909398aa6e18de11fb2517eb06cc0e",
    "permalink": "https://g-tierney.github.io/post/name_graphs/",
    "publishdate": "2018-08-25T00:00:00Z",
    "relpermalink": "/post/name_graphs/",
    "section": "post",
    "summary": "How to efficiently standardize names and link observations from different sources using graphs.",
    "tags": [
      "Record Linkage",
      "Fencing",
      "Graphs",
      "Networks"
    ],
    "title": "Record linkage: An Adventure in Graph Theory",
    "type": "post"
  },
  {
    "authors": null,
    "categories": [
      "Article Review"
    ],
    "content": " This is my second article round up. The first is over at my old blog here. In this post, I’ll briefly cover two Atlantic articles related to economic inequality and the differing economic expectations of democrats and republicans. Most time is spent reviewing a new working paper on the correlation between Trump’s twitter activity and hate crimes, and thinking about how it could make the jump from correlation to causation.\nIncome Inequality Matthew Stuart’s “The 9.9 Percent Is the New American Aristocracy” and Jordan Weissman’s reply “Actually, the 1 Percent Are Still the Problem”\nThe first article is quite long, but easily skim-able. It focuses on not the super wealthy but the elite professionals that make up the upper class and the pernicious ways that group has convinced itself that membership is meritocratic when in reality parental wealth is inherited to a high degree. The various mechanism for this inheritance are in the article (knowing what preschools to go to, what SAT tutors to hire, etc.), but it also pointed out that this group (to which I belong) often refuses to acknowledge that they are upper class and do not represent the “average American.” And that self-delusion of meritocracy can be quite dangerous when it is used to justify the status quo.\nWeissman’s response makes the valid point that the top 90th percentile to the 99.9th percentile of the income distribution are quite heterogeneous: the elite professionals are in there but also are old retirees. And he points out that the difference in wealth and privilege between the 90th and 99th percentiles is quite large. Therefore, the 9.9% are not even a cohesive group, let alone “the New American Aristocracy.” Often when reading about a subject I’m not personally knowledgeable on, I find myself agreeing with whoever I’m reading. This Slate article pushed back on the data and factual claims in a way that I wouldn’t have been able to do myself just assessing the analytical arguments.\n Partisan Economies Annie Lowrey’s “Left Economy, Right Economy”\nAn interesting look into how the partisan change in post-election economic expectations hasn’t disappeared after 2016. The article goes into how democrats and republicans have radically different economic expectations. This is one of the more easily measured areas of where partisan beliefs can impact non-political opinions. They also cite some good research that “politically-motivated” beliefs about the direction of the economy don’t impact consumer spending, indicating that perhaps the people espousing the motivated beliefs know that they are untrue.\nOn some level, this shouldn’t be surprising. Democrats and republicans disagree on the empirical question of which policies improve the economy. On the other hand, to the extent that economies aren’t that impacted by government policies, it is a huge collection of people looking at the same data and coming to opposite conclusions.\n Trump Tweets and Hate Crimes Karsten Müller and Carlo Schwarz’s “Making America Hate Again? Twitter and Hate Crime Under Trump”\nA lot has been written about whether the 2016 election and Trump’s campaign emboldened racists. A new working paper from economists at the University of Warwick took a look at whether Trumps activity on Twitter is related to an increase in hate crimes. The abstract is below.\n Abstract: Social media has come under increasing scrutiny for reinforcing people’s pre-existing viewpoints which, it is argued, can create information “echo chambers.” We investigate whether social media motivates real-life action, with a focus on hate crimes in the United States. We show that the rise in anti-Muslim hate crimes since Donald Trump’s presidential campaign has been concentrated in counties with high Twitter usage. Consistent with a role for social media, Trump’s Tweets on Islam-related topics are highly correlated with anti-Muslim hate crime after, but not before the start of his presidential campaign, and are uncorrelated with other types of hate crimes. These patterns stand out in historical comparison: counties with many Twitter users today did not consistently experience more anti-Muslim hate crimes during previous presidencies.\n This is the main figure:   Figure 2: Panel (a) shows the weekly number of Donald Trump’s  Islam-related tweets and the number of anti-Muslim hate crimes  in the US after the start of Trump’s presidential campaign.  \nSome other key results:\n “Trump’s Muslim tweets alone predict more than 20% of the variation in anti-Muslim hate crimes in the same week, but only after his campaign start; the explanatory power is less than 1% before.”\n “[T]he number of hate crimes [in counties with low and high twitter usage] was more or less constant since 2009. With the start of Donald Trump’s presidential campaign on June, 16th 2015, however, we observe a disproportional increase in the number of hate crimes in those counties where many people use Twitter. There is no comparable increase in counties with low twitter usage.”\n “In additional exercises, reported in Supplementary Material 6, we find complementary results for Hispanics. While the association weakens somewhat in the immediate run-up to the election in mid-2016, Table A.10 and Table A.11 show that Trump’s tweets about Hispanics have considerable predictive power for Ethnicity-based hate crimes. Again, this only holds true for the period after his campaign start, and not for other types of crime biases.”\n  The correlation is quite striking. It kind of makes me wish that the Twitter employee who deleted Trump’s account on their last day would come back to quit every day (or at least pick random days to delete the account).\nI thought this paper was really interesting for a couple of reasons. Foremost, it gets about as close as possible to claiming and showing that a correlation is causal. They do many, many robustness checks that, while imperfect, all indicate that alternative explanations are unlikely. Of course, correlation does not imply causation in the mathematical sense, but it does imply it in the colloquial sense. If Trump’s anti-Muslim tweets are in response to world events that would cause an increase in hate crimes absent his tweets, you would expect a similar correlation between his tweets and hate crimes before his presidential run. But that correlation isn’t observed. If high twitter usage counties are more prone to hate crimes, you would expect elevated levels of hate crimes before he became president. But that correlation also isn’t observed. Other counter arguments to the (unstated) causal claim are similarly addressed.\nIf the paper did try to make a causal claim (which it does not), the appropriate counter-factual is an interesting discussion. The counter-factual of anti-Muslim hate crimes under a Clinton presidency is the most relevant for a judgement on Trump’s presidency, but hard to estimate. The counter-factual of Trump wants to send an anti-Muslim tweet but does not or cannot for some reason is less relevant, but much easier to estimate. Some ways to get at this counter-factual would be to identify times when Trump was not able to tweet. I’m not sure if he gets service on Air Force One, but time spent traveling or in meetings could be used as an instrument for his Islam-related tweets. Unfortunately, the time periods he is otherwise occupied with presidential work are probably too short and infrequent. Long periods while he is overseas, he might tweet at odd hours, so the exposure of Twitter to his tweets could be plausibly thought of as exogenously lowered. Essentially these methods try to find “random” reasons that Twitter was more or less less exposed to Trump, then use the variation in Trump’s Islam-related tweets attributable to those random shocks to explain variation in hate crimes. That final explanation is causal because the ultimate source of the variation is random.\nAnother way to get at the counter-factual is to try to identify “random” tweets, i.e. anti-Muslim tweets not caused by other events. This kind of analysis has been explored in finance when doing event studies of how unexpected news affects a company’s stock price. Reviews of each of his at-issue tweets and a selection of events that could cause an increase in anti-Muslim hate crimes could identify events and tweets that are linked (such as the travel ban and resulting tweets), events that were not tweeted, and tweets not associated with events. The PredictIt markets for the number of tweets he sends each week could even be used to create “expected” levels of Twitter activity, then deviations from that expectation could be treated as unexpected shocks. Of course, that relies on the strong assumption that PredictIt Twitter markets are efficient. Choosing the events to analyze would be hard too, but you could potentially algorithmically identify events that sparked many tweets related to Islam in the US and select from those.\nThis last method gets at what I think is probably the true causal structure. Trump’s election and campaign emboldened people who harbored racial resentment and increased the baseline rate of hate crimes. Trump’s tweets are caused by news-worthy events that would have received coverage on traditional and social media. Those events and the coverage would have caused an increase in hate crimes regardless of his tweets. When he does choose to tweet about them, he increases their exposure and amplifies their impact on anti-Muslim hate crimes. Separating events and tweets into three categories (event-tweet pairs, unpaired events, and unpaired tweets) and comparing before and after Trump’s campaign began can fully test at least the correlations implied by this theory. To the degree that the pairing and non-pairing of tweets and events is exogenous, the correlations become causal as well. That exogeneity isn’t something that can be statistically tested but can come from a qualitative analysis of the time surrounding the event and/or tweet.\nThe paper is going through the peer review process, so what comes out the other side may be quite different. It does a lot of interesting analysis and adds to an important discussion about the consequences of the 2016 election. I certainly do not expect this to be the last paper on the impact of Trump on racial resentment in America.\n ",
    "date": 1528329600,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1528329600,
    "objectID": "0c8b22c41ef962256c14d2da1575ebd4",
    "permalink": "https://g-tierney.github.io/post/2018_06_best_reads/",
    "publishdate": "2018-06-07T00:00:00Z",
    "relpermalink": "/post/2018_06_best_reads/",
    "section": "post",
    "summary": "A review of some interesting articles regarding who are the true economic elites, partisan economic forecasts, and the effect of Trump's tweets on hate crimes.",
    "tags": [
      "Economics",
      "Politics",
      "Social Media"
    ],
    "title": "Article Round Up June 2018: Income Inequality, Partisan Economies, and Trump Tweets to Hate Crimes",
    "type": "post"
  },
  {
    "authors": null,
    "categories": [
      "Data Analysis"
    ],
    "content": "  Introduction Last spring, I took a class on Bayesian statistics at the University of Chicago that had several exercises focused on building a model to classify species based on their genome. The basic setup was that you were given a data set of salmon, their genome sequencing data, and which sub-population they belonged to. From this data, we needed to build a model to classify new salmon into the sub-populations. The strategy was to use the fact that alleles appeared with different frequencies in the different sub-populations, so the fact that a new fish did have certain alleles and did not have others was informative about which sub-population it came from.\nThis problem and solution struck me as being very similar to an issue near and dear to my heart: classifying decks into archetypes in collectible card games (CCGs). Specifically, Magic the Gathering. One could think of alleles as analogues for cards and the sub-populations as analogues for deck archetypes (UW Control, Jund, Merfolk, etc.). In this post, I will describe how to apply a Bayesian classification algorithm to this scenario and discuss some of its advantages. I assume the reader has familiarity with basic mathematical probability and statistics. I also will not show proofs for the mathematical claims that I make, but I will try to explain the intuition behind the concepts. Knowing the application area, collectible card games and Magic, will be necessary to understand the examples.\nUltimately, with very minimal data cleaning, I am able to correctly classify 80% of decks played in a testing sample. Frequently played archetypes and archetypes that share few cards with others are more accurately classified. The code and data are available on Github. I also describe some simple human-implementable improvements and additional features that should be included in practice.\n The Problem Magic, and collectible card games broadly, are usually two-player games where each player brings their own deck of different cards. Many tournament-caliber decks are categorized into broader archetypes that differ only slightly. For example, Merfolk is a deck that plays creature cards with the Merfolk creature type and “lords” that give bonuses to all Merfolk cards. Not all Merfolk decks are identical, but a human who knows the game well can easily look at a deck-list and say if it is a Merfolk deck or not. Jund is a deck that plays green, black, and red cards that focus on trading resources with the opponent while eking out small advantages in each exchange. The variety of cards played in Jund is significantly higher than the variety of cards played in Merfolk decks.\nClassifying a single deck into an archetype is an easy task for a human who knows the game well, but frequently this classification has to be done at scale. Large tournaments happen every weekend, thousands of matches are played online every day, and it would take an extremely large team to classify all of those decks. A classification algorithm would ideally give a data-driven, suggested archetype and express how uncertain it was about that suggestion.\nI know of two types of practitioners who face this problem. First are the companies that make collectible card games. They are often interested in assessing whether there is a sufficiently diverse array of successful archetypes, which requires efficiently classifying decks then calculating things like play- and win-rates by archetype. The developers can then either ban or change existing cards to improve the format. Second are third-party websites. They often provide “meta-game reports” that cover which archetypes are successful or likely to become successful. For them, the value of archetype statistics is simply to report them to players who have to decide which deck to bring to a tournament.\n The Data The method I will describe relies on having a lot of decks with known archetypes. I will use data from MTG Goldfish on Modern decks played in Star City Games (SCG) events from 2014 to the present. Modern is a non-rotating format, which means that cards from older sets will always be legal in the format. In a rotating format like Standard, where only cards from sets released in the past two years are legal, data on older decks is not useful to classify newer decks. Additionally, the names of deck archetypes at Star City Games events are more standardized than the weekly data from Magic Online (MTGO), the digital version of the game. The algorithm could be used in these other scenarios with enough data, but deck-lists are only released for top finishers at SCG events and on MTGO.\nOver the 841 tournaments, I observe 8,249 unique decks, 516 unique deck archetypes, and 1,772 unique cards. I will separate a 300 deck sample to use for testing after constructing the model.\n The Method I’ll start by defining some notation. \\(D_{new}\\) is a new deck that needs to be classified into an archetype. It is a list of card names and card quantities. It looks like this:\n  card  number      Experiment One  4    Goblin Guide  4    Kird Ape  4    Wild Nacatl  4    Burning-Tree Emissary  4    Tarmogoyf  4     We also have a set of training decks. The same card name and card quantity variables are included, but it also includes the true archetype name. The ultimate goal is to use the information in the training decks to classify a new deck of an unknown archetype.\nMaximum Likelihood The simplest approach to the classification problem is to use a maximum likelihood estimation. In the training set, find the frequency that each card \\(c\\) appears among cards in archetype \\(i\\) and treat this as the probability that a random card from archetype \\(i\\) will be card \\(c\\). For example, 1,653 Lightning Bolts appear among the 26,363 cards in Jund decks, so this probability would be estimated as 0.06. The likelihood that \\(D_{new}\\) came from archetype \\(i\\) can be thought of as the probability that 60 draws with replacement from a pool of every Magic card will result in \\(D_{new}\\) when the probability of drawing each card is given by the probabilities estimated for archetype \\(i\\) in the training data. Formally, this is a draw of size 60 from a Multinomial distribution on the population of every Magic card and probability parameters \\(p_i = \u0026lt;p_{i,1},...,p_{i,1772}\u0026gt;\\) where \\(p_{i,c}\\) is the proportion of card \\(c\\) among cards in archetype \\(i\\). This is written below (note that \\(x_c\\) is the number of times card \\(c\\) appeared in \\(D_{new}\\)):\n\\[P(D_{new}|\\text{archetype}=i) = \\frac{60!}{x_1! ... x_{1772}!} \\prod_{c=i}^{1772}p_{c,i}^{x_c} \\propto \\prod_{c=i}^{1772}p_{c,i}^{x_c} \\]\nThe most relevant part of this probability is the large product of each card frequency for each time it appears in \\(D_{new}\\). The factorials just count the number of ways \\(D_{new}\\) could have been drawn. However, it can be completely ignored! We will classify \\(D_{new}\\) as whatever archetype maximizes the likelihood. The factorials are the same for every archetype, so ignoring them will not change the result.\nThe biggest drawback of this method is that if a card is never observed in a given archetype in the training data, the estimated likelihood that any deck containing that card is of that archetype is zero. This can be particularly problematic. A blue-white control deck that decides to use Baneslayer Angel as a finisher might have 59 cards identical to a blue-white control deck in the training set, but it will have a likelihood of zero if no training blue-white control deck contained Baneslayer Angel.\nThe solution to this problem is to add pseudo counts to every deck in the training set. Suppose you round every zero frequency to some small number, say 0.01. Now if a new card appears in an archetype, the likelihood will not be zero, and if the rest of the deck matches a known archetype well, it can still be correctly classified. Picking this pseudo count number can be hard. It should maybe even be different for different decks based on how many times that archetype is observed in the training set because you are likely more confident in zero frequencies for decks that you observe many times.\n Bayesian Modeling The method I propose is essentially a more rigorous way of adding these pseudo counts by putting a Bayesian prior on the frequencies. A useful distribution on a set of frequencies is the Dirichlet distribution. A draw from an n-dimensional Dirichlet distribution is a set of n positive numbers that sum to one. Note that n-1 dimensions identify the sample because the last dimension must ensure the values all sum to one. It has n parameters, call each \\(\\alpha_c\\), and the expected value for \\(p_c\\), the frequency of card \\(c\\), is \\(\\alpha_c/\\sum_{j=1}^n \\alpha_j\\). As \\(\\alpha_c\\) increases, the variance decreases. To understand this distribution, its useful to first consider the two-dimensional case, called a Beta distribution. The link shows some useful visualizations of the distribution under different parameters. The Beta(1,1) distribution is uniform, Beta(5,5) has a peak at 0.5, Beta(1,4) has a peak at 0.25, Beta(0.1,0.1) has a minimum at 0.5 and has asymptotic behavior at 0 and 1.\nSuppose we start with a uniform Dirichlet prior for each deck (\\(\\alpha_c = 1\\) for all \\(c\\)), essentially starting from the position that all frequency combinations are equally likely. The intuition is that the model starts from the perspective that, without any data, the probability that a random draw of a single card from an archetype \\(i\\) is a specific card \\(c\\) is equal to \\(1/n\\), where \\(n\\) is the number of unique cards (1,772 in my data). The probability that a random card from an Infect deck is Lightning Bolt is 1/1,772, the probability that a random card from a Tron deck is Urza’s Mine is 1/1,772, etc. The uncertainty about these probabilities is also the same for every card in every archetype. Of course these probabilities are wrong in practice, but they provide a sensible starting point and will be updated with data.\nNext, we observe the training data for each archetype, \\(D_t\\). Assuming that each archetype has a unique set of card frequencies, using proper Bayesian updating, the posterior distribution of frequencies (the distribution conditional on the data) for each archetype is also Dirichlet with parameters \\(\\alpha_c = 1 + n_c\\), the number of times card \\(c\\) appeared in the archetype across all decks in \\(D_t\\). That is, starting from the prior stated above and given the training data, beliefs about the true card frequencies for each archetype are described by this distribution. To understand the intuition, after observing the training data, the probability that a single random card drawn from archetype \\(i\\) is equal to a specific card \\(c\\) is (1 + the number of card \\(c\\) that appeared in archetype \\(i\\) (\\(n_{c,i}\\)))/(\\(n\\) + the number of cards from archetype \\(i\\) (\\(n_i\\))). For example, Lightning Bolt was never observed in an Infect deck, so the posterior expected probability that a random card from an Infect deck is Lightning Bolt is \\((1+0)/(1,772+21,607) \\approx 0.00013\\). Four copies of Urza’s Mine were observed in every Tron deck, so the posterior expected probability that a random card from a Tron deck is Urza’s Mine is \\((1+1,272)/(1,772+19,099) \\approx 0.061\\), which is pretty close to \\(4/60 = 0.0\\overline{6}\\). Using this method, an Infect deck splashing Lightning Bolt would not be ruled impossible, just extremely unlikely, and the model has determined that nearly every Tron deck has four Urza’s Mines. The posterior also captures differing levels of confidence in these estimations. The variance in frequencies for archetypes observed many times in \\(D_t\\) is lower than for archetypes observed fewer times because the variance decreases as the sum of the \\(\\alpha\\) parameters increases. In other words, I am more confident in the estimation of the frequency of Urza’s Mine in Tron, a heavily played deck, than I am about the frequency of Blood Moon in Skred Red decks, a rarely played deck.\nFinally, when encountering a new deck, we want to compute a posterior probability that the deck came from each of the archetypes we saw in the training data. Adopting a prior that without data each archetype is equally probable implies that the likelihood alone will determine this probability. A new deck, \\(D_{new}\\), is a collection of 60 cards defined by a vector of \\(x_c\\)’s, the number of times card \\(c\\) appears in \\(D_{new}\\). Our model interprets this deck as a draw of size 60 from a Multinomial-Dirichlet distribution on the set of all unique cards with Dirichlet parameters set by the posterior described above. This distribution is a Multinomial distribution where the frequency parameters are an unknown draw from a known Dirichlet distribution. So, for each archetype, the likelihood is computed as as:\n\\[P(\\text{archetype} = i | D_{new},D_t,\\alpha=1) \\propto P(D_{new} | \\text{archetype} = i,D_t,\\alpha=1) = \\] \\[f(D_{new};\\text{size} = 60,\\alpha_c = 1+n_{c,i}) = \\frac{(60!) \\Gamma(\\sum_{c=1}^{1772} \\alpha_c)}{\\Gamma(60 + \\sum_{c=1}^{1772} \\alpha_c)} \\prod_{c=1}^{1772} \\frac{x_c + \\alpha_c}{(x_c!) \\Gamma(\\alpha_c)} \\]\nWhere \\(f(x)\\) is the pmf of the Multinomial-Dirichlet distribution and \\(\\Gamma\\) is the gamma function, which is very similar to a factorial that can be applied non-integers. This function does look a lot more complicated than the simple likelihood before. That is because it is accounting for the uncertainty in the frequencies. You could simplify this function by using just the expectation of each frequency for each deck (\\(\\alpha_c\\) divided by the sum of the \\(\\alpha_c\\)’s). That likelihood would look like the simple likelihood described previously where you can ignore the factorial constant and just take the product of each expected probability exponentiated by \\(x_c\\). It would solve the problem of zero frequency cards. However, you would lose information about how confident you were in each expectation. The estimated frequencies of an archetype only observed twice in \\(D_t\\) should be viewed with more suspicion than the frequencies for one observed 500 times. That is the reason the gamma functions outside of the product cannot be ignored in this likelihood.\nAfter computing each of these, standardize them to sum to one and you have the probability that the new deck came from each of the observed archetypes. Classify it into whichever archetype has the highest probability and you’re done!\n  Results Available on Github is R code that implements the above method, allowing for a flexible specification of \\(\\alpha\\) in the prior. I chose \\(\\alpha=1\\) because it represents starting from a uniform distribution, but the choice of prior is certainly open for debate. I also show the results for \\(\\alpha=0\\), which corresponds to the initial maximum likelihood setup where zero frequencies are possible.\nThe results for \\(\\alpha\\) equal to 0, 1, and 0.01 are reported below. \\(\\alpha\\) of 0.01 corresponds to a prior with a smaller effect on the posterior (each \\(\\alpha_c\\) will be \\(0.01 + n_{c,i}\\)) and frequencies closer to zero are more likely. Correct is the number of correct classifications, Size is the number of decks classified, and Rate is the ratio of those two. The Confident columns subset the results to classifications were the posterior probability of the suggested classification is greater than 95%.\n  Alpha  Correct  Size  Rate  Correct (Confident)  Size (Confident)  Rate (Confident)      0.00  1  300  0.00  0  0  NaN    0.01  235  300  0.78  224  278  0.81    1.00  239  300  0.80  236  292  0.81     The value of \\(\\alpha=1\\) appears to be the most accurate, getting 80% of classifications correct, but the smaller \\(\\alpha\\) is not very different. It is also worth noting that the method appears to be too confident in its classifications. One would hope the accuracy rate would be similar to the estimated probability, but it appears that the classifications made with at least 95% probability have an accuracy rate well below that number.\nI will look at the accuracy rate by archetype as well. The table below shows the five most frequent and five least frequent archetypes observed in the testing data. Total is the number of times the archetype appears, Proportion Correct is the proportion of those decks that are correctly classified, and Mode Incorrect is the most common incorrect classification (missing values indicate that all decks are correctly classified).\n  Deck  Total  Proportion Correct  Mode Incorrect      Infect  19  1.00     Jund  17  1.00     Naya Burn  15  0.93  Burn    Tron  15  1.00     Affinity  14  1.00     Wr  1  0.00  Wr Prison    Wr Control  1  0.00  Wr Prison    Wr Prison  1  0.00  Rw Nahiri    Wrg  1  0.00  Naya Zoo    Wu Control  1  0.00  Uw Control     Infect, Tron, and Affinity are very unique decks, so it is not surprising that the algorithm correctly classifies them. Jund is very similar to many other black green midrange decks, so I was surprised they were all correctly identified. That could be because Jund is a very common archetype in the training decks, so the algorithm has enough data to separate Jund from similar decks. Naya Burn was most frequently miss classified as normal Burn, which is not very surprising.\nAmong the least frequent archetypes, the mistakes are not unexpected. Its possible that WR, WR Control, and WR Prison should be the same archetype anyway, and the difference between WU Control versus UW Control is just which color is more prevalent.\n Human Improvements I think a lot of the time, people want their statistical model to work without any human input. However, human-level tweaks often provide significantly greater performance improvements than tweaks to the statistical methods. Three areas where an analyst with domain knowledge could improve the method are outlined below.\nFirst, standardizing deck archetypes. I made a few changes to these but wanted to leave the data mostly raw. For example, I made “U/R Twin” and “UR Twin” the same deck, changed “Death \u0026amp; Taxes” to “Death And Taxes”, and standardized capitalization. Several remaining archetypes, however, should probably be grouped together. Some low hanging fruit are probably the 38 “Naya Through the Breach” decks and the 21 “Naya Titan Breach” decks, and the 88 “UB Tezzerator” and 26 “UB Tezzeret” decks. These kind of changes could be made incrementally as more data are added by standardizing the names of new decks and by someone who was unfamiliar with the code and statistics, but was familiar with the Modern format.\nAnother area an analyst could improve these results is by selectively including true zeros in the likelihood. For example, Abzan compared to Abzan Company decks differ by whether the card Collected Company is included in the list. With enough data, the algorithm will learn this distinction, but that identification could be expedited and accuracy increased if the Abzan likelihood function had a zero frequency for Collected Company. A similar method could be used to separate similar archetypes defined by color splashes, such as distinguishing Jeskai Twin, Grixis Twin, and UR Twin by including zeros for lands by which color they produce.\nA third and final area more domain knowledge could help is manually reviewing low probability classifications. Let’s look at the most uncertain classifications. These six decks are the ones the algorithm had the most uncertainty about. An analyst could review these decks to separate archetypes that are quite similar.\n  Deck  Classification  Probability Correct  Correct      Burn  Burn  0.59  TRUE    Mono-Blue Grand Architect  Mono-Blue Turns  0.74  FALSE    Tasigur Burn  Unknown  0.77  FALSE    Mono-Blue Grand Architect  Mono-Blue Turns  0.84  FALSE    Mono-White Human Aggro  Mono-White Humans  0.85  FALSE    Wr Prison  Rw Nahiri  0.90  FALSE      Implementation in Practice A company or data scientist building this tool for frequent use in an analysis pipeline would want to design a few features beyond the one-time estimation and classification described here.\nTraining data updating. CCGs evolve over time with new card releases and novel combinations discovered by players. When a new archetype is formed, examples of it need to be added to the training data. To that end, and to update old archetypes with new cards, samples of decks should be regularly classified by humans and added to the training data, with special emphasis placed on getting sufficient samples of new archetypes. It would also be wise to sample more heavily from decks with low probability classifications and track accuracy rates of the new training decks.\nArchetype hierarchies. Many of the incorrect classifications were from mistaking sub-archetypes, such as confusing Naya Burn for Burn or Wilt-Leaf Abzan for Abzan. A two-stage classification could be more accurate and better address the questions practitioners are asking. An analyst would place some archetypes into categories, all Burn decks into one category and all Abzan into another. The algorithm would be implemented twice. Once to place a deck into a category then again to place the deck within that category. The higher-level category will probably be more accurate and sufficient for most practitioners’ purposes.\nPrior selection. The uniform prior of \\(\\alpha = 1\\) is useful for interpretation, but practitioners may find it to be less accurate for archetypes with small sample sizes. Selecting the \\(\\alpha\\) that maximizes the accuracy rate in the training data is one simple option, or a cross-validation method could pick \\(\\alpha\\) to maximize the accuracy rate across many samples.\nIncorporating game-specific features. Every CCG has unique deck-building rules, which could be built into the likelihood function. Magic decks cannot contain more than four of a single card and no fewer than 60 total cards, so frequencies greater than 4/60 are impossible. In Hearthstone, the cap is two for some cards, one for others, and decks must be exactly 30 cards. Deck size is already included as a parameter in the Multinomial-Dirichlet distribution, but card limits are not. Two alternatives I tried were treating each deck as a series of Bernoulli random variables indicating whether a card was present or not in a deck and treating a deck as a draw from several Multinomial distributions of size 4 (or the appropriate card limit). They were not as accurate as the method described here, but potentially could be improved. At the least, they are worth experimenting with on new data sets.\n Machine Learning Methods I am certain that there are machine learning methods that address this problem as well. Classification is a well-studied topic in machine learning and this classification problem does not present too many unique challenges. However, I think this Bayesian approach has certain advantages in model updating and uncertainty quantification. It is quite easy to update the model with new, correctly-labeled decks. Simply add the new card counts the appropriate archetype in the training data. Many ML methods would need to re-calibrate tuning and regularization parameters with new training data, which could be quite time consuming. The second advantage is uncertainty quantification. Given the prior, this model explicitly reports the probability that the classification is correct. This is useful in flagging cases for manual review, as noted above.\n Conclusion While not perfect, this algorithm should become quite adept at identifying archetypes with enough data. In formats without much turnover in the top decks, collecting the amount of data required is not very difficult. In rotating formats, however, it could be much harder. The people interested in this kind of application, however, might have more than just the top decks from weekend tournaments. Wizards of the Coast, the company that makes Magic, can observe every game played on Magic Online, tournament organizers collect deck lists from every player, and Vicious Syndicate has a popular deck tracking app for Hearthstone. They can certainly collect enough samples of decks, and perhaps the method described here could improve their classification accuracy or reduce the human labor required.\n ",
    "date": 1526515200,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1526515200,
    "objectID": "1e00a5a6953a838b52d934dec91da339",
    "permalink": "https://g-tierney.github.io/post/magic_classification/",
    "publishdate": "2018-05-17T00:00:00Z",
    "relpermalink": "/post/magic_classification/",
    "section": "post",
    "summary": "An application of genetic classification algorithms to collectible card games.",
    "tags": [
      "Bayesian",
      "Classification",
      "Genetics"
    ],
    "title": "The Genetics of Magic",
    "type": "post"
  },
  {
    "authors": null,
    "categories": null,
    "content": "",
    "date": 1461733200,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1461733200,
    "objectID": "5de5e556cd553dcad22633be528fdf8b",
    "permalink": "https://g-tierney.github.io/teaching/political_text/",
    "publishdate": "2016-04-27T00:00:00-05:00",
    "relpermalink": "/teaching/political_text/",
    "section": "teaching",
    "summary": "Myself and [another Ph.D. student](https://beckytang.rbind.io/) created a module to teach undergraduates statistical methods for text analysis focused on political applications. We created and taught variants for implementation in introductory and advanced classes. This work was funded by the Duke Rhodes Information Initiative and is documented here: https://bigdata.duke.edu/projects/text-analysis-political-speech.",
    "tags": [
      "demo"
    ],
    "title": "Political Text Analysis",
    "type": "teaching"
  }
]