<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Graham Tierney on Graham Tierney</title>
    <link>https://g-tierney.github.io/</link>
    <description>Recent content in Graham Tierney on Graham Tierney</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Data Science in Mental Health</title>
      <link>https://g-tierney.github.io/post/2018_09_mental_health/</link>
      <pubDate>Thu, 20 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://g-tierney.github.io/post/2018_09_mental_health/</guid>
      <description>&lt;p&gt;I came across two articles recently that I thought spoke to each other in an interesting way. The first was a New York Times piece about the failings of data science firms who try to identify school shootings before they happen by social media posts. The second was a Vox article about how a crisis counseling hotline successfully used data science to flag callers who are at higher risk of suicide or self-harm.&lt;/p&gt;
&lt;p&gt;I have some specific thoughts on both, but I think the comparison between the two articles shows why a data-driven approach is helpful in one case and not the other.&lt;/p&gt;
&lt;div id=&#34;a-failure-predicting-school-shootings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A Failure: Predicting School Shootings&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.nytimes.com/2018/09/06/us/social-media-monitoring-school-shootings.html&#34; target=&#34;_blank&#34;&gt;“Could Monitoring Students on Social Media Stop the Next School Shooting?” by Aaron Leibowitz&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This article reviews the services that several companies are providing to school districts by monitoring public posts by students on social media. These companies usually scrape data from all posts in a geographic region around the school. “Rather than asking schools for a list of students and social media handles, the companies typically employ a method called “geofencing” to sweep up posts within a given geographic area and use keywords to narrow the pool.”&lt;/p&gt;
&lt;p&gt;However, as you can imagine, this wide of a net ends up flagging posts from many people unaffiliated with the school. One school in Ohio was warned about someone posting “There’s three seasons: summer, construction season and school shooting season.” Further investigation discovered that the poster was from Wisconsin not Ohio. Another school that hired one of these firms was close to a liquor store, and the firm couldn’t separate tweets about the store and the school. In general, the problem seems to be that in the available data, any signals, if they exist, are swamped by the noise.&lt;/p&gt;
&lt;p&gt;This monitoring also brought up some philosophical questions about what out-of-school actions should have in-school consequences. A case of a school that hired an outside firm to review students’ posts on social media expelled 14 students. Some of the allegations are described below:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;One student had been accused of “holding too much money” in photographs, an investigation by the Southern Poverty Law Center found, and one was suspended for an Instagram post in which she wore a sweatshirt with an airbrushed image of her father, a murder victim. School officials said the sweatshirt’s colors and the student’s hand symbol were evidence of gang ties, according to the investigation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I can understand an administrator’s desire to punish students for some out-of-school actions, but these seem like they’ve gone too far. School officials who suddenly have access to &lt;em&gt;all&lt;/em&gt; public activity of their students need to think harder about what kind of actions they want to police. Especially if they want to continue to monitor students’ activities for more serious transgressions, officials need to be tolerant of activities they may disapprove of to keep that information channel open. I’m sure that many fewer students made any public posts after these expulsions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-success-identifying-high-risk-callers-to-a-crisis-hotline&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A Success: Identifying High-Risk Callers to a Crisis Hotline&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.vox.com/science-and-health/2018/6/8/17441452/suicide-prevention-anthony-bourdain-crisis-text-line-data-science&#34; target=&#34;_blank&#34;&gt;“How data scientists are using AI for suicide prevention” by Brian Resnick&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A more heartening case is this article about the data science team at Crisis Text Line. CTL provides crisis counseling via text message to anyone who requests it. Certain events cause dramatic increases in demand for their services; Robin Williams’s suicide and the 2015 terrorist attack in Paris are two examples. The volunteers working at the time cannot handle everyone at once, so they used machine learning to prioritize incoming requests based on the text message rather than using the order the requests came in. The words most predictive of an active rescue (when 911 is called) were the names of household drugs like Advil or Ibuprofen, even the crying face emoji was more predictive than the word “suicide” and other words like “cut” and “kill” that the company had thought would be good predictors previously.&lt;/p&gt;
&lt;p&gt;Something I wondered throughout this piece was what really was the “machine learning” being used and did it really rise to the level of artificial intelligence? It sounds like the analysis could have been simply computed by a simple logistic regression of active rescue on indicators for which words were used in the first message. It sounds a bit pedantic, but I think overuse of buzzwords like machine learning and AI discourage people who would have valuable insights or could produce data analysis of similar rigor and results from looking into these problems. For a longer read on how data is being used (and not used) in the counseling profession, check out this &lt;a href=&#34;https://www.theatlantic.com/magazine/archive/2017/04/what-your-therapist-doesnt-know/517797/&#34;&gt;Atlantic piece&lt;/a&gt; that was linked in the Vox article.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-was-the-difference&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What was the difference?&lt;/h2&gt;
&lt;p&gt;Both CTL and the school shooting programs were trying to predict individual behavior from limited textual data. I think the difference is that the event they wanted to detect was much more frequent and observable in the studied population.&lt;/p&gt;
&lt;p&gt;My intuition is that the proportion of school shooters among everyone living near a school is significantly smaller than the proportion of callers to a crisis hotline who need an active rescue. The most credible estimates are that in the 2015-2016 school year there were &lt;a href=&#34;https://theconversation.com/why-theres-so-much-inconsistency-in-school-shooting-data-102318&#34; target=&#34;_blank&#34;&gt;11 to 29&lt;/a&gt; school shootings across the country. The data on social media posts by the shooters has to be even more scarce. Statistical methods to identify which features are predictive of a given event need data from when the event does and does not occur. The school security firms, lacking many data on what shooters post before they bring a gun to school, ended up simply referring “violent-sounding” messages to school officials, without being able to specify how likely that person was to actually be violent at school. That determination was left up to school officials who, given discretion, appear to have made some poor choices about what kinds of messages merited a response. I would not be surprised if part of the justification school administrators had in mind was that the data science firm flagged the message because they thought the person would be violent at school. When in reality, the firm isn’t doing any data-based prediction on what messages correlate with actual violence.&lt;/p&gt;
&lt;p&gt;CTL, however, had a clear outcome variable and data on both callers who did and did not need active rescue. They were able to build a statistical model and identify predictive features of the incoming messages to effectively allocate resources. CTL had the appropriate labeled data, but the schools only had a limited selection of messages from students who had not yet (and might never be) violent at school.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Record linkage: An Adventure in Graph Theory</title>
      <link>https://g-tierney.github.io/post/name_graphs/</link>
      <pubDate>Sat, 25 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://g-tierney.github.io/post/name_graphs/</guid>
      <description>&lt;script src=&#34;https://g-tierney.github.io/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I recently encountered a problem that had a surprisingly elegant solution. I struggled a lot with solving this issue, so hopefully in writing this post I can save someone else the trouble! For reasons that are irrelevant, I wanted to track the performance of youth fencers across time. National ranking lists are posted each year, but the fencers’ names frequently change from year to year. My solution was to create a dataset of all sets of two names, mark which pairs were matches, then pick a single name to use for each set of names that matched against each other. My mistaken belief was that the second step would be the hardest, but actually the third was the most difficult.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Problem&lt;/h2&gt;
&lt;p&gt;The data I had were end-of-season rankings for youth fencers by age, gender, and weapon. The only identifier to link fencers across years was their name and year of birth. However, the names are notoriously difficult to standardize. Some years a kid goes by John, others its Jonathan, and maybe a third time its John Smith IV. Numeric suffixes seemed quite overrepresented among fencers relative to the general population. Some Asian fencers would go by a phonetic spelling of their given name in one year then use an English name in the next.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;solution-part-1-the-surprisingly-easy-part&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Solution Part 1: The (Surprisingly) Easy Part&lt;/h2&gt;
&lt;p&gt;I joined the list of names (along with the other less granular identifiers) to itself on the less granular identifiers to get a list of all pairs of names that had the same age, gender, and weapon. I removed pairs whose last names required more than three deletions, insertions, or single-letter transformations to match (the Levenshtein distance). I then tasked two undergraduate RAs to independently inspect each pair and mark the ones that could be the same name, then resolve any discrepancies in the matches they found.&lt;/p&gt;
&lt;p&gt;I had thought this step, reviewing each pair of names, would be the most time-consuming, but it actually was rather quick. The 11,211 were reviewed and mismatched in less than 8 hours of work per RA. A snapshot of the final dataset is shown below.&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
match
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
name_key
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
name_key2
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
gender
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
weapon
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
bthyear
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
aaron.ahn
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
albert.park
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Male
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
foil
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1997
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
aaron.ahn
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
albert.park
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Male
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
foil
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1997
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
aaron.ahn
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
bin.ahn
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Male
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
foil
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1997
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
aaron.ahn
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
royce.wang
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Male
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
foil
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1997
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
aaron.ahn
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
mikolaj.bak
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Male
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
foil
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1997
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
aaron.ahn
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
eric.zhang
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Male
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
foil
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1997
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;solution-part-2-the-hard-part&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Solution Part 2: The Hard Part&lt;/h2&gt;
&lt;p&gt;After this dataset of matches was created, I needed to identify for each name, all other names that matched with it, then pick one of those names to use as the “real” name. The operations required to do this were surprisingly challenging. Each name appeared in both name_key columns, so any grouping had to be done on two variables. It also requires a consistent operation that will select the same “real” name for each name within the matches. I was struggling to implement this solution on the rectangular dataframe. I needed to group by names in two variables, spread unique values of two variables into multiple columns, then consistently select one of those entries. Certainly, there is a way to do this, but it was not intuitive to me and I suspect it would be quite a slow operation.&lt;/p&gt;
&lt;p&gt;Eventually, I realized that instead of trying to operate on the data as a matrix with variables in columns and observations in rows, I should treat the data as a graph. Each name was a node, and edges represent names that were matched. Each connected graph within the disconnected graph of all names represented a single “real” name. To extract what I needed, I just had list each node and which graph it was in, then arbitrarily pick one node from each graph to be the “real” name.&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Fortunately, people who write much better R code than me have developed tools to operate on graphs quickly and efficiently. What took me hours to (unsuccessfully) do on a rectangular dataframe took approximately 30 minutes using graph operations. A quick visualization of the graph is below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;graph_plot &amp;lt;- suppressWarnings(graph.data.frame(name_matches[1:100,]  %&amp;gt;% filter(match == 1) %&amp;gt;% select(name_key,name_key2),directed = F))

#remove duplicated edges
graph_plot &amp;lt;- graph_plot %&amp;gt;% simplify() 

set.seed(0515) #fix the position of nodes on the plot
ggnet2(graph_plot,label = T,layout.exp=2,color = &amp;quot;lightskyblue2&amp;quot;) + 
  ggtitle(&amp;quot;Sample Name Network&amp;quot;) + theme(plot.title = element_text(hjust = .5,face = &amp;quot;bold&amp;quot;,size = 15))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://g-tierney.github.io/post/name_graphs_files/figure-html/plot_graph-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now here is the code itself. The full script and example data are available at &lt;a href=&#34;https://github.com/g-tierney/record_linkage_graphs&#34;&gt;this Github repository&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(igraph)

#add other identifiers to node names
combine_ids &amp;lt;- function(...){
  str_c(...,sep = &amp;quot;_&amp;quot;)
}
name_matches &amp;lt;- name_matches %&amp;gt;% rowwise() %&amp;gt;% 
  mutate(name_key = combine_ids(name_key,gender,weapon,bthyear),
         name_key2 = combine_ids(name_key2,gender,weapon,bthyear)) 

#turn data into a graph
graph &amp;lt;- graph.data.frame(name_matches %&amp;gt;% filter(match == 1) %&amp;gt;% select(name_key,name_key2),directed = F)
dg &amp;lt;- decompose.graph(graph)

#list names of verticies grouped by connected graphs
name_links &amp;lt;- map(dg,function(x){V(x)$name})

#combine into a single dataframe to merge
make_df &amp;lt;- function(list_element){
  vec &amp;lt;- unlist(list_element)
  std_name &amp;lt;- str_split(vec[1],&amp;quot;_&amp;quot;,simplify = T)[1]
  data.frame(name_key_combined = vec,std_name = std_name,stringsAsFactors = F)
}
name_standardizations &amp;lt;- do.call(rbind,map(name_links,make_df))

#spread identifiers back into multiple columns
name_standardizations &amp;lt;- name_standardizations %&amp;gt;% separate(name_key_combined,into = c(&amp;quot;name_key&amp;quot;,&amp;quot;gender&amp;quot;,&amp;quot;weapon&amp;quot;,&amp;quot;bthyear&amp;quot;),sep = &amp;quot;_&amp;quot;)
name_standardizations[1:7,] %&amp;gt;% kableExtra::kable(format = &amp;quot;html&amp;quot;) %&amp;gt;% 
  kableExtra::kable_styling(bootstrap_options = &amp;quot;striped&amp;quot;,full_width = F)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
name_key
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
gender
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
weapon
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
bthyear
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
std_name
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
abagael.a.buckborough
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Female
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
sabre
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1999
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
abagael.a.buckborough
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
abagael.r.buckborough
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Female
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
sabre
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1999
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
abagael.a.buckborough
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
abby.buckborough
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Female
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
sabre
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1999
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
abagael.a.buckborough
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
abby.emerson
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Female
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
foil
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1987
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
abby.emerson
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
abigail.emerson
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Female
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
foil
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1987
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
abby.emerson
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
abby.schifferle
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Female
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
foil
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1988
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
abby.schifferle
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
abigail.schifferle
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Female
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
foil
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
1988
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
abby.schifferle
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The lesson I learned from this experience was the importance of taking a step back from a difficult problem and approaching it from a different angle. Not thinking of data as a matrix of numbers was instrumental to solving this particular problem and is likely key to solving many others.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Incidentally, I think the reason I came up with this idea was I was researching professors at Ph.D. programs I was accepted to, and one of them, Rebecca Steorts, mentioned research on record linkage using graphs in her research interests.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Article Round Up June 2018: Income Inequality, Partisan Economies, and Trump Tweets to Hate Crimes</title>
      <link>https://g-tierney.github.io/post/2018_06_best_reads/</link>
      <pubDate>Thu, 07 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://g-tierney.github.io/post/2018_06_best_reads/</guid>
      <description>&lt;p&gt;This is my second article round up. The first is over at &lt;a href=&#34;https://ticktocksaythehandsoftheclock.wordpress.com/2018/04/06/march-2018-universities-fake-news-and-drugs/&#34;&gt;my old blog here&lt;/a&gt;. In this post, I’ll briefly cover two Atlantic articles related to economic inequality and the differing economic expectations of democrats and republicans. Most time is spent reviewing a new working paper on the correlation between Trump’s twitter activity and hate crimes, and thinking about how it could make the jump from correlation to causation.&lt;/p&gt;
&lt;div id=&#34;income-inequality&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Income Inequality&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Matthew Stuart’s &lt;a href=&#34;https://www.theatlantic.com/magazine/archive/2018/06/the-birth-of-a-new-american-aristocracy/559130/&#34;&gt;“The 9.9 Percent Is the New American Aristocracy”&lt;/a&gt; and Jordan Weissman’s reply &lt;a href=&#34;https://slate.com/business/2018/05/forget-the-atlantics-9-9-percent-the-1-percent-are-still-the-problem.html&#34;&gt;“Actually, the 1 Percent Are Still the Problem”&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first article is quite long, but easily skim-able. It focuses on not the super wealthy but the elite professionals that make up the upper class and the pernicious ways that group has convinced itself that membership is meritocratic when in reality parental wealth is inherited to a high degree. The various mechanism for this inheritance are in the article (knowing what preschools to go to, what SAT tutors to hire, etc.), but it also pointed out that this group (to which I belong) often refuses to acknowledge that they are upper class and do not represent the “average American.” And that self-delusion of meritocracy can be quite dangerous when it is used to justify the status quo.&lt;/p&gt;
&lt;p&gt;Weissman’s response makes the valid point that the top 90th percentile to the 99.9th percentile of the income distribution are quite heterogeneous: the elite professionals are in there but also are old retirees. And he points out that the difference in wealth and privilege between the 90th and 99th percentiles is quite large. Therefore, the 9.9% are not even a cohesive group, let alone “the New American Aristocracy.” Often when reading about a subject I’m not personally knowledgeable on, I find myself agreeing with whoever I’m reading. This Slate article pushed back on the data and factual claims in a way that I wouldn’t have been able to do myself just assessing the analytical arguments.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;partisan-economies&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Partisan Economies&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Annie Lowrey’s &lt;a href=&#34;https://www.theatlantic.com/politics/archive/2018/06/two-economies/561929/&#34;&gt;“Left Economy, Right Economy”&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;An interesting look into how the partisan change in post-election economic expectations hasn’t disappeared after 2016. The article goes into how democrats and republicans have radically different economic expectations. This is one of the more easily measured areas of where partisan beliefs can impact non-political opinions. They also cite some good research that “politically-motivated” beliefs about the direction of the economy don’t impact consumer spending, indicating that perhaps the people espousing the motivated beliefs know that they are untrue.&lt;/p&gt;
&lt;p&gt;On some level, this shouldn’t be surprising. Democrats and republicans disagree on the empirical question of which policies improve the economy. On the other hand, to the extent that economies aren’t that impacted by government policies, it is a huge collection of people looking at the same data and coming to opposite conclusions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;trump-tweets-and-hate-crimes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Trump Tweets and Hate Crimes&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Karsten Müller and Carlo Schwarz’s &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3149103&#34;&gt;“Making America Hate Again? Twitter and Hate Crime Under Trump”&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A lot has been written about whether the 2016 election and Trump’s campaign emboldened racists. A new working paper from economists at the University of Warwick took a look at whether Trumps activity on Twitter is related to an increase in hate crimes. The abstract is below.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt;: Social media has come under increasing scrutiny for reinforcing people’s pre-existing viewpoints which, it is argued, can create information “echo chambers.” We investigate whether social media motivates real-life action, with a focus on hate crimes in the United States. We show that the rise in anti-Muslim hate crimes since Donald Trump’s presidential campaign has been concentrated in counties with high Twitter usage. Consistent with a role for social media, Trump’s Tweets on Islam-related topics are highly correlated with anti-Muslim hate crime after, but not before the start of his presidential campaign, and are uncorrelated with other types of hate crimes. These patterns stand out in historical comparison: counties with many Twitter users today did not consistently experience more anti-Muslim hate crimes during previous presidencies.&lt;/p&gt;
&lt;/blockquote&gt;
This is the main figure:
&lt;center&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://g-tierney.github.io/img/tweets_hate_crimes.png&#34; /&gt;

&lt;/div&gt;
Figure 2: Panel (a) shows the weekly number of Donald Trump’s &lt;br/&gt; Islam-related tweets and the number of anti-Muslim hate crimes &lt;br/&gt; in the US after the start of Trump’s presidential campaign.
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Some other key results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;“Trump’s Muslim tweets alone predict more than 20% of the variation in anti-Muslim hate crimes in the same week, but only after his campaign start; the explanatory power is less than 1% before.”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;“[T]he number of hate crimes [in counties with low and high twitter usage] was more or less constant since 2009. With the start of Donald Trump’s presidential campaign on June, 16th 2015, however, we observe a disproportional increase in the number of hate crimes in those counties where many people use Twitter. There is no comparable increase in counties with low twitter usage.”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;“In additional exercises, reported in Supplementary Material 6, we find complementary results for Hispanics. While the association weakens somewhat in the immediate run-up to the election in mid-2016, Table A.10 and Table A.11 show that Trump’s tweets about Hispanics have considerable predictive power for Ethnicity-based hate crimes. Again, this only holds true for the period after his campaign start, and not for other types of crime biases.”&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The correlation is quite striking. It kind of makes me wish that the &lt;a href=&#34;https://www.npr.org/sections/thetwo-way/2017/11/03/561770603/twitter-employee-blamed-for-deleting-presidents-account&#34;&gt;Twitter employee who deleted Trump’s account on their last day&lt;/a&gt; would come back to quit every day (or at least pick random days to delete the account).&lt;/p&gt;
&lt;p&gt;I thought this paper was really interesting for a couple of reasons. Foremost, it gets about as close as possible to claiming and showing that a correlation is causal. They do many, many robustness checks that, while imperfect, all indicate that alternative explanations are unlikely. Of course, correlation does not imply causation in the mathematical sense, but it does imply it in the colloquial sense. If Trump’s anti-Muslim tweets are in response to world events that would cause an increase in hate crimes absent his tweets, you would expect a similar correlation between his tweets and hate crimes before his presidential run. But that correlation isn’t observed. If high twitter usage counties are more prone to hate crimes, you would expect elevated levels of hate crimes before he became president. But that correlation also isn’t observed. Other counter arguments to the (unstated) causal claim are similarly addressed.&lt;/p&gt;
&lt;p&gt;If the paper did try to make a causal claim (which it does not), the appropriate counter-factual is an interesting discussion. The counter-factual of anti-Muslim hate crimes under a Clinton presidency is the most relevant for a judgement on Trump’s presidency, but hard to estimate. The counter-factual of Trump wants to send an anti-Muslim tweet but does not or cannot for some reason is less relevant, but much easier to estimate. Some ways to get at this counter-factual would be to identify times when Trump was not able to tweet. I’m not sure if he gets service on Air Force One, but time spent traveling or in meetings could be used as an instrument for his Islam-related tweets. Unfortunately, the time periods he is otherwise occupied with presidential work are probably too short and infrequent. Long periods while he is overseas, he might tweet at odd hours, so the exposure of Twitter to his tweets could be plausibly thought of as exogenously lowered. Essentially these methods try to find “random” reasons that Twitter was more or less less exposed to Trump, then use the variation in Trump’s Islam-related tweets attributable to those random shocks to explain variation in hate crimes. That final explanation is causal because the ultimate source of the variation is random.&lt;/p&gt;
&lt;p&gt;Another way to get at the counter-factual is to try to identify “random” tweets, i.e. anti-Muslim tweets not caused by other events. This kind of analysis has been explored in finance when doing event studies of how unexpected news affects a company’s stock price. Reviews of each of his at-issue tweets and a selection of events that could cause an increase in anti-Muslim hate crimes could identify events and tweets that are linked (such as the travel ban and resulting tweets), events that were not tweeted, and tweets not associated with events. The &lt;a href=&#34;https://www.predictit.org/home/browse?Search=tweets&amp;amp;isSearch=true&#34;&gt;PredictIt markets for the number of tweets he sends each week&lt;/a&gt; could even be used to create “expected” levels of Twitter activity, then deviations from that expectation could be treated as unexpected shocks. Of course, that relies on the strong assumption that PredictIt Twitter markets are efficient. Choosing the events to analyze would be hard too, but you could potentially algorithmically identify events that sparked many tweets related to Islam in the US and select from those.&lt;/p&gt;
&lt;p&gt;This last method gets at what I think is probably the true causal structure. Trump’s election and campaign emboldened people who harbored racial resentment and increased the baseline rate of hate crimes. Trump’s tweets are caused by news-worthy events that would have received coverage on traditional and social media. Those events and the coverage would have caused an increase in hate crimes regardless of his tweets. When he does choose to tweet about them, he increases their exposure and amplifies their impact on anti-Muslim hate crimes. Separating events and tweets into three categories (event-tweet pairs, unpaired events, and unpaired tweets) and comparing before and after Trump’s campaign began can fully test at least the correlations implied by this theory. To the degree that the pairing and non-pairing of tweets and events is exogenous, the correlations become causal as well. That exogeneity isn’t something that can be statistically tested but can come from a qualitative analysis of the time surrounding the event and/or tweet.&lt;/p&gt;
&lt;p&gt;The paper is going through the peer review process, so what comes out the other side may be quite different. It does a lot of interesting analysis and adds to an important discussion about the consequences of the 2016 election. I certainly do not expect this to be the last paper on the impact of Trump on racial resentment in America.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Genetics of Magic</title>
      <link>https://g-tierney.github.io/post/magic_classification/</link>
      <pubDate>Thu, 17 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://g-tierney.github.io/post/magic_classification/</guid>
      <description>&lt;script src=&#34;https://g-tierney.github.io/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Last spring, I took a class on Bayesian statistics at the University of Chicago that had several exercises focused on building a model to classify species based on their genome. The basic setup was that you were given a data set of salmon, their genome sequencing data, and which sub-population they belonged to. From this data, we needed to build a model to classify new salmon into the sub-populations. The strategy was to use the fact that alleles appeared with different frequencies in the different sub-populations, so the fact that a new fish did have certain alleles and did not have others was informative about which sub-population it came from.&lt;/p&gt;
&lt;p&gt;This problem and solution struck me as being very similar to an issue near and dear to my heart: classifying decks into archetypes in collectible card games (CCGs). Specifically, Magic the Gathering. One could think of alleles as analogues for cards and the sub-populations as analogues for deck archetypes (UW Control, Jund, Merfolk, etc.). In this post, I will describe how to apply a Bayesian classification algorithm to this scenario and discuss some of its advantages. I assume the reader has familiarity with basic mathematical probability and statistics. I also will not show proofs for the mathematical claims that I make, but I will try to explain the intuition behind the concepts. Knowing the application area, collectible card games and Magic, will be necessary to understand the examples.&lt;/p&gt;
&lt;p&gt;Ultimately, with very minimal data cleaning, I am able to correctly classify 80% of decks played in a testing sample. Frequently played archetypes and archetypes that share few cards with others are more accurately classified. The code and data are available on &lt;a href=&#34;https://github.com/g-tierney/magic_deck_classification_multi_dir&#34;&gt;Github&lt;/a&gt;. I also describe some simple human-implementable improvements and additional features that should be included in practice.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Problem&lt;/h2&gt;
&lt;p&gt;Magic, and collectible card games broadly, are usually two-player games where each player brings their own deck of different cards. Many tournament-caliber decks are categorized into broader archetypes that differ only slightly. For example, Merfolk is a deck that plays creature cards with the Merfolk creature type and “lords” that give bonuses to all Merfolk cards. Not all Merfolk decks are identical, but a human who knows the game well can easily look at a deck-list and say if it is a Merfolk deck or not. Jund is a deck that plays green, black, and red cards that focus on trading resources with the opponent while eking out small advantages in each exchange. The variety of cards played in Jund is significantly higher than the variety of cards played in Merfolk decks.&lt;/p&gt;
&lt;p&gt;Classifying a single deck into an archetype is an easy task for a human who knows the game well, but frequently this classification has to be done at scale. Large tournaments happen every weekend, thousands of matches are played online every day, and it would take an extremely large team to classify all of those decks. A classification algorithm would ideally give a data-driven, suggested archetype and express how uncertain it was about that suggestion.&lt;/p&gt;
&lt;p&gt;I know of two types of practitioners who face this problem. First are the companies that make collectible card games. They are often interested in assessing whether there is a sufficiently diverse array of successful archetypes, which requires efficiently classifying decks then calculating things like play- and win-rates by archetype. The developers can then either ban or change existing cards to improve the format. Second are third-party websites. They often provide “meta-game reports” that cover which archetypes are successful or likely to become successful. For them, the value of archetype statistics is simply to report them to players who have to decide which deck to bring to a tournament.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Data&lt;/h2&gt;
&lt;p&gt;The method I will describe relies on having a lot of decks with known archetypes. I will use data from &lt;a href=&#34;https://www.mtggoldfish.com/&#34;&gt;MTG Goldfish&lt;/a&gt; on Modern decks played in Star City Games (SCG) events from 2014 to the present. Modern is a non-rotating format, which means that cards from older sets will always be legal in the format. In a rotating format like Standard, where only cards from sets released in the past two years are legal, data on older decks is not useful to classify newer decks. Additionally, the names of deck archetypes at Star City Games events are more standardized than the weekly data from Magic Online (MTGO), the digital version of the game. The algorithm could be used in these other scenarios with enough data, but deck-lists are only released for top finishers at SCG events and on MTGO.&lt;/p&gt;
&lt;p&gt;Over the 841 tournaments, I observe 8,249 unique decks, 516 unique deck archetypes, and 1,772 unique cards. I will separate a 300 deck sample to use for testing after constructing the model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Method&lt;/h2&gt;
&lt;p&gt;I’ll start by defining some notation. &lt;span class=&#34;math inline&#34;&gt;\(D_{new}\)&lt;/span&gt; is a new deck that needs to be classified into an archetype. It is a list of card names and card quantities. It looks like this:&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
card
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
number
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Experiment One
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Goblin Guide
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Kird Ape
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wild Nacatl
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Burning-Tree Emissary
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tarmogoyf
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We also have a set of training decks. The same card name and card quantity variables are included, but it also includes the true archetype name. The ultimate goal is to use the information in the training decks to classify a new deck of an unknown archetype.&lt;/p&gt;
&lt;div id=&#34;maximum-likelihood&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Maximum Likelihood&lt;/h3&gt;
&lt;p&gt;The simplest approach to the classification problem is to use a maximum likelihood estimation. In the training set, find the frequency that each card &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; appears among cards in archetype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and treat this as the probability that a random card from archetype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; will be card &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;. For example, 1,653 Lightning Bolts appear among the 26,363 cards in Jund decks, so this probability would be estimated as 0.06. The likelihood that &lt;span class=&#34;math inline&#34;&gt;\(D_{new}\)&lt;/span&gt; came from archetype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; can be thought of as the probability that 60 draws with replacement from a pool of every Magic card will result in &lt;span class=&#34;math inline&#34;&gt;\(D_{new}\)&lt;/span&gt; when the probability of drawing each card is given by the probabilities estimated for archetype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; in the training data. Formally, this is a draw of size 60 from a Multinomial distribution on the population of every Magic card and probability parameters &lt;span class=&#34;math inline&#34;&gt;\(p_i = &amp;lt;p_{i,1},...,p_{i,1772}&amp;gt;\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(p_{i,c}\)&lt;/span&gt; is the proportion of card &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; among cards in archetype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. This is written below (note that &lt;span class=&#34;math inline&#34;&gt;\(x_c\)&lt;/span&gt; is the number of times card &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; appeared in &lt;span class=&#34;math inline&#34;&gt;\(D_{new}\)&lt;/span&gt;):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(D_{new}|\text{archetype}=i) = \frac{60!}{x_1! ... x_{1772}!} \prod_{c=i}^{1772}p_{c,i}^{x_c} \propto \prod_{c=i}^{1772}p_{c,i}^{x_c} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The most relevant part of this probability is the large product of each card frequency for each time it appears in &lt;span class=&#34;math inline&#34;&gt;\(D_{new}\)&lt;/span&gt;. The factorials just count the number of ways &lt;span class=&#34;math inline&#34;&gt;\(D_{new}\)&lt;/span&gt; could have been drawn. However, it can be completely ignored! We will classify &lt;span class=&#34;math inline&#34;&gt;\(D_{new}\)&lt;/span&gt; as whatever archetype maximizes the likelihood. The factorials are the same for every archetype, so ignoring them will not change the result.&lt;/p&gt;
&lt;p&gt;The biggest drawback of this method is that if a card is never observed in a given archetype in the training data, the estimated likelihood that any deck containing that card is of that archetype is zero. This can be particularly problematic. A blue-white control deck that decides to use Baneslayer Angel as a finisher might have 59 cards identical to a blue-white control deck in the training set, but it will have a likelihood of zero if no training blue-white control deck contained Baneslayer Angel.&lt;/p&gt;
&lt;p&gt;The solution to this problem is to add pseudo counts to every deck in the training set. Suppose you round every zero frequency to some small number, say 0.01. Now if a new card appears in an archetype, the likelihood will not be zero, and if the rest of the deck matches a known archetype well, it can still be correctly classified. Picking this pseudo count number can be hard. It should maybe even be different for different decks based on how many times that archetype is observed in the training set because you are likely more confident in zero frequencies for decks that you observe many times.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayesian-modeling&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bayesian Modeling&lt;/h3&gt;
&lt;p&gt;The method I propose is essentially a more rigorous way of adding these pseudo counts by putting a Bayesian prior on the frequencies. A useful distribution on a set of frequencies is the Dirichlet distribution. A draw from an n-dimensional Dirichlet distribution is a set of n positive numbers that sum to one. Note that n-1 dimensions identify the sample because the last dimension must ensure the values all sum to one. It has n parameters, call each &lt;span class=&#34;math inline&#34;&gt;\(\alpha_c\)&lt;/span&gt;, and the expected value for &lt;span class=&#34;math inline&#34;&gt;\(p_c\)&lt;/span&gt;, the frequency of card &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;, is &lt;span class=&#34;math inline&#34;&gt;\(\alpha_c/\sum_{j=1}^n \alpha_j\)&lt;/span&gt;. As &lt;span class=&#34;math inline&#34;&gt;\(\alpha_c\)&lt;/span&gt; increases, the variance decreases. To understand this distribution, its useful to first consider the two-dimensional case, called a &lt;a href=&#34;https://stephens999.github.io/fiveMinuteStats/beta.html&#34;&gt;Beta distribution&lt;/a&gt;. The link shows some useful visualizations of the distribution under different parameters. The Beta(1,1) distribution is uniform, Beta(5,5) has a peak at 0.5, Beta(1,4) has a peak at 0.25, Beta(0.1,0.1) has a minimum at 0.5 and has asymptotic behavior at 0 and 1.&lt;/p&gt;
&lt;p&gt;Suppose we start with a uniform Dirichlet prior for each deck (&lt;span class=&#34;math inline&#34;&gt;\(\alpha_c = 1\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;), essentially starting from the position that all frequency combinations are equally likely. The intuition is that the model starts from the perspective that, without any data, the probability that a random draw of a single card from an archetype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is a specific card &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is equal to &lt;span class=&#34;math inline&#34;&gt;\(1/n\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the number of unique cards (1,772 in my data). The probability that a random card from an Infect deck is Lightning Bolt is 1/1,772, the probability that a random card from a Tron deck is Urza’s Mine is 1/1,772, etc. The uncertainty about these probabilities is also the same for every card in every archetype. Of course these probabilities are wrong in practice, but they provide a sensible starting point and will be updated with data.&lt;/p&gt;
&lt;p&gt;Next, we observe the training data for each archetype, &lt;span class=&#34;math inline&#34;&gt;\(D_t\)&lt;/span&gt;. Assuming that each archetype has a unique set of card frequencies, using proper Bayesian updating, the posterior distribution of frequencies (the distribution conditional on the data) for each archetype is also Dirichlet with parameters &lt;span class=&#34;math inline&#34;&gt;\(\alpha_c = 1 + n_c\)&lt;/span&gt;, the number of times card &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; appeared in the archetype across all decks in &lt;span class=&#34;math inline&#34;&gt;\(D_t\)&lt;/span&gt;. That is, starting from the prior stated above and given the training data, beliefs about the true card frequencies for each archetype are described by this distribution. To understand the intuition, after observing the training data, the probability that a single random card drawn from archetype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is equal to a specific card &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is (1 + the number of card &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; that appeared in archetype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(n_{c,i}\)&lt;/span&gt;))/(&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; + the number of cards from archetype &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; (&lt;span class=&#34;math inline&#34;&gt;\(n_i\)&lt;/span&gt;)). For example, Lightning Bolt was never observed in an Infect deck, so the posterior expected probability that a random card from an Infect deck is Lightning Bolt is &lt;span class=&#34;math inline&#34;&gt;\((1+0)/(1,772+21,607) \approx 0.00013\)&lt;/span&gt;. Four copies of Urza’s Mine were observed in every Tron deck, so the posterior expected probability that a random card from a Tron deck is Urza’s Mine is &lt;span class=&#34;math inline&#34;&gt;\((1+1,272)/(1,772+19,099) \approx 0.061\)&lt;/span&gt;, which is pretty close to &lt;span class=&#34;math inline&#34;&gt;\(4/60 = 0.0\overline{6}\)&lt;/span&gt;. Using this method, an Infect deck splashing Lightning Bolt would not be ruled impossible, just extremely unlikely, and the model has determined that nearly every Tron deck has four Urza’s Mines. The posterior also captures differing levels of confidence in these estimations. The variance in frequencies for archetypes observed many times in &lt;span class=&#34;math inline&#34;&gt;\(D_t\)&lt;/span&gt; is lower than for archetypes observed fewer times because the variance decreases as the sum of the &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; parameters increases. In other words, I am more confident in the estimation of the frequency of Urza’s Mine in Tron, a heavily played deck, than I am about the frequency of Blood Moon in Skred Red decks, a rarely played deck.&lt;/p&gt;
&lt;p&gt;Finally, when encountering a new deck, we want to compute a posterior probability that the deck came from each of the archetypes we saw in the training data. Adopting a prior that without data each archetype is equally probable implies that the likelihood alone will determine this probability. A new deck, &lt;span class=&#34;math inline&#34;&gt;\(D_{new}\)&lt;/span&gt;, is a collection of 60 cards defined by a vector of &lt;span class=&#34;math inline&#34;&gt;\(x_c\)&lt;/span&gt;’s, the number of times card &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; appears in &lt;span class=&#34;math inline&#34;&gt;\(D_{new}\)&lt;/span&gt;. Our model interprets this deck as a draw of size 60 from a Multinomial-Dirichlet distribution on the set of all unique cards with Dirichlet parameters set by the posterior described above. This distribution is a Multinomial distribution where the frequency parameters are an unknown draw from a known Dirichlet distribution. So, for each archetype, the likelihood is computed as as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P(\text{archetype} = i | D_{new},D_t,\alpha=1) \propto P(D_{new} | \text{archetype} = i,D_t,\alpha=1) = \]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[f(D_{new};\text{size} = 60,\alpha_c = 1+n_{c,i}) = \frac{(60!) \Gamma(\sum_{c=1}^{1772} \alpha_c)}{\Gamma(60 + \sum_{c=1}^{1772} \alpha_c)} \prod_{c=1}^{1772} \frac{x_c + \alpha_c}{(x_c!) \Gamma(\alpha_c)} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; is the pmf of the Multinomial-Dirichlet distribution and &lt;span class=&#34;math inline&#34;&gt;\(\Gamma\)&lt;/span&gt; is the gamma function, which is very similar to a factorial that can be applied non-integers. This function does look a lot more complicated than the simple likelihood before. That is because it is accounting for the uncertainty in the frequencies. You could simplify this function by using just the expectation of each frequency for each deck (&lt;span class=&#34;math inline&#34;&gt;\(\alpha_c\)&lt;/span&gt; divided by the sum of the &lt;span class=&#34;math inline&#34;&gt;\(\alpha_c\)&lt;/span&gt;’s). That likelihood would look like the simple likelihood described previously where you can ignore the factorial constant and just take the product of each expected probability exponentiated by &lt;span class=&#34;math inline&#34;&gt;\(x_c\)&lt;/span&gt;. It would solve the problem of zero frequency cards. However, you would lose information about how confident you were in each expectation. The estimated frequencies of an archetype only observed twice in &lt;span class=&#34;math inline&#34;&gt;\(D_t\)&lt;/span&gt; should be viewed with more suspicion than the frequencies for one observed 500 times. That is the reason the gamma functions outside of the product cannot be ignored in this likelihood.&lt;/p&gt;
&lt;p&gt;After computing each of these, standardize them to sum to one and you have the probability that the new deck came from each of the observed archetypes. Classify it into whichever archetype has the highest probability and you’re done!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;p&gt;Available on &lt;a href=&#34;https://github.com/g-tierney/magic_deck_classification_multi_dir&#34;&gt;Github is R code&lt;/a&gt; that implements the above method, allowing for a flexible specification of &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; in the prior. I chose &lt;span class=&#34;math inline&#34;&gt;\(\alpha=1\)&lt;/span&gt; because it represents starting from a uniform distribution, but the choice of prior is certainly open for debate. I also show the results for &lt;span class=&#34;math inline&#34;&gt;\(\alpha=0\)&lt;/span&gt;, which corresponds to the initial maximum likelihood setup where zero frequencies are possible.&lt;/p&gt;
&lt;p&gt;The results for &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; equal to 0, 1, and 0.01 are reported below. &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; of 0.01 corresponds to a prior with a smaller effect on the posterior (each &lt;span class=&#34;math inline&#34;&gt;\(\alpha_c\)&lt;/span&gt; will be &lt;span class=&#34;math inline&#34;&gt;\(0.01 + n_{c,i}\)&lt;/span&gt;) and frequencies closer to zero are more likely. Correct is the number of correct classifications, Size is the number of decks classified, and Rate is the ratio of those two. The Confident columns subset the results to classifications were the posterior probability of the suggested classification is greater than 95%.&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Alpha
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Correct
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Size
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Rate
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Correct (Confident)
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Size (Confident)
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Rate (Confident)
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
300
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
NaN
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.01
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
235
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
300
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.78
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
224
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
278
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.81
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
239
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
300
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.80
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
236
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
292
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.81
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The value of &lt;span class=&#34;math inline&#34;&gt;\(\alpha=1\)&lt;/span&gt; appears to be the most accurate, getting 80% of classifications correct, but the smaller &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; is not very different. It is also worth noting that the method appears to be too confident in its classifications. One would hope the accuracy rate would be similar to the estimated probability, but it appears that the classifications made with at least 95% probability have an accuracy rate well below that number.&lt;/p&gt;
&lt;p&gt;I will look at the accuracy rate by archetype as well. The table below shows the five most frequent and five least frequent archetypes observed in the testing data. Total is the number of times the archetype appears, Proportion Correct is the proportion of those decks that are correctly classified, and Mode Incorrect is the most common incorrect classification (missing values indicate that all decks are correctly classified).&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Deck
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Total
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Proportion Correct
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Mode Incorrect
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Infect
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
19
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Jund
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
17
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Naya Burn
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.93
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Burn
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tron
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Affinity
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
14
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wr
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wr Prison
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wr Control
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wr Prison
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wr Prison
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Rw Nahiri
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wrg
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Naya Zoo
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wu Control
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.00
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Uw Control
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Infect, Tron, and Affinity are very unique decks, so it is not surprising that the algorithm correctly classifies them. Jund is very similar to many other black green midrange decks, so I was surprised they were all correctly identified. That could be because Jund is a very common archetype in the training decks, so the algorithm has enough data to separate Jund from similar decks. Naya Burn was most frequently miss classified as normal Burn, which is not very surprising.&lt;/p&gt;
&lt;p&gt;Among the least frequent archetypes, the mistakes are not unexpected. Its possible that WR, WR Control, and WR Prison should be the same archetype anyway, and the difference between WU Control versus UW Control is just which color is more prevalent.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;human-improvements&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Human Improvements&lt;/h2&gt;
&lt;p&gt;I think a lot of the time, people want their statistical model to work without any human input. However, human-level tweaks often provide significantly greater performance improvements than tweaks to the statistical methods. Three areas where an analyst with domain knowledge could improve the method are outlined below.&lt;/p&gt;
&lt;p&gt;First, standardizing deck archetypes. I made a few changes to these but wanted to leave the data mostly raw. For example, I made “U/R Twin” and “UR Twin” the same deck, changed “Death &amp;amp; Taxes” to “Death And Taxes”, and standardized capitalization. Several remaining archetypes, however, should probably be grouped together. Some low hanging fruit are probably the 38 “Naya Through the Breach” decks and the 21 “Naya Titan Breach” decks, and the 88 “UB Tezzerator” and 26 “UB Tezzeret” decks. These kind of changes could be made incrementally as more data are added by standardizing the names of new decks and by someone who was unfamiliar with the code and statistics, but was familiar with the Modern format.&lt;/p&gt;
&lt;p&gt;Another area an analyst could improve these results is by selectively including true zeros in the likelihood. For example, Abzan compared to Abzan Company decks differ by whether the card Collected Company is included in the list. With enough data, the algorithm will learn this distinction, but that identification could be expedited and accuracy increased if the Abzan likelihood function had a zero frequency for Collected Company. A similar method could be used to separate similar archetypes defined by color splashes, such as distinguishing Jeskai Twin, Grixis Twin, and UR Twin by including zeros for lands by which color they produce.&lt;/p&gt;
&lt;p&gt;A third and final area more domain knowledge could help is manually reviewing low probability classifications. Let’s look at the most uncertain classifications. These six decks are the ones the algorithm had the most uncertainty about. An analyst could review these decks to separate archetypes that are quite similar.&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Deck
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Classification
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
Probability Correct
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Correct
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Burn
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Burn
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.59
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TRUE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mono-Blue Grand Architect
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mono-Blue Turns
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.74
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Tasigur Burn
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Unknown
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.77
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mono-Blue Grand Architect
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mono-Blue Turns
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.84
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mono-White Human Aggro
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mono-White Humans
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.85
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Wr Prison
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Rw Nahiri
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.90
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FALSE
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;implementation-in-practice&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Implementation in Practice&lt;/h2&gt;
&lt;p&gt;A company or data scientist building this tool for frequent use in an analysis pipeline would want to design a few features beyond the one-time estimation and classification described here.&lt;/p&gt;
&lt;p&gt;Training data updating. CCGs evolve over time with new card releases and novel combinations discovered by players. When a new archetype is formed, examples of it need to be added to the training data. To that end, and to update old archetypes with new cards, samples of decks should be regularly classified by humans and added to the training data, with special emphasis placed on getting sufficient samples of new archetypes. It would also be wise to sample more heavily from decks with low probability classifications and track accuracy rates of the new training decks.&lt;/p&gt;
&lt;p&gt;Archetype hierarchies. Many of the incorrect classifications were from mistaking sub-archetypes, such as confusing Naya Burn for Burn or Wilt-Leaf Abzan for Abzan. A two-stage classification could be more accurate and better address the questions practitioners are asking. An analyst would place some archetypes into categories, all Burn decks into one category and all Abzan into another. The algorithm would be implemented twice. Once to place a deck into a category then again to place the deck within that category. The higher-level category will probably be more accurate and sufficient for most practitioners’ purposes.&lt;/p&gt;
&lt;p&gt;Prior selection. The uniform prior of &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 1\)&lt;/span&gt; is useful for interpretation, but practitioners may find it to be less accurate for archetypes with small sample sizes. Selecting the &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; that maximizes the accuracy rate in the training data is one simple option, or a cross-validation method could pick &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; to maximize the accuracy rate across many samples.&lt;/p&gt;
&lt;p&gt;Incorporating game-specific features. Every CCG has unique deck-building rules, which could be built into the likelihood function. Magic decks cannot contain more than four of a single card and no fewer than 60 total cards, so frequencies greater than 4/60 are impossible. In Hearthstone, the cap is two for some cards, one for others, and decks must be exactly 30 cards. Deck size is already included as a parameter in the Multinomial-Dirichlet distribution, but card limits are not. Two alternatives I tried were treating each deck as a series of Bernoulli random variables indicating whether a card was present or not in a deck and treating a deck as a draw from several Multinomial distributions of size 4 (or the appropriate card limit). They were not as accurate as the method described here, but potentially could be improved. At the least, they are worth experimenting with on new data sets.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;machine-learning-methods&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Machine Learning Methods&lt;/h2&gt;
&lt;p&gt;I am certain that there are machine learning methods that address this problem as well. Classification is a well-studied topic in machine learning and this classification problem does not present too many unique challenges. However, I think this Bayesian approach has certain advantages in model updating and uncertainty quantification. It is quite easy to update the model with new, correctly-labeled decks. Simply add the new card counts the appropriate archetype in the training data. Many ML methods would need to re-calibrate tuning and regularization parameters with new training data, which could be quite time consuming. The second advantage is uncertainty quantification. Given the prior, this model explicitly reports the probability that the classification is correct. This is useful in flagging cases for manual review, as noted above.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;While not perfect, this algorithm should become quite adept at identifying archetypes with enough data. In formats without much turnover in the top decks, collecting the amount of data required is not very difficult. In rotating formats, however, it could be much harder. The people interested in this kind of application, however, might have more than just the top decks from weekend tournaments. Wizards of the Coast, the company that makes Magic, can observe every game played on Magic Online, tournament organizers collect deck lists from every player, and Vicious Syndicate has a popular deck tracking app for Hearthstone. They can certainly collect enough samples of decks, and perhaps the method described here could improve their classification accuracy or reduce the human labor required.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
